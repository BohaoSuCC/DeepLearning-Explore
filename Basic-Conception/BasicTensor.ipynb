{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parent:\n",
    "    def __init__(self, value):\n",
    "        self.value = value\n",
    "        print(f\"Parent initialized with value {self.value}\")\n",
    "\n",
    "class Child(Parent):\n",
    "    def __init__(self, value, extra):\n",
    "        super().__init__(value)  # 调用父类的 __init__ 方法\n",
    "        self.extra = extra\n",
    "        print(f\"Child initialized with value {self.value} and extra {self.extra}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parent initialized with value 10\n",
      "Child initialized with value 10 and extra 20\n"
     ]
    }
   ],
   "source": [
    "Child.value = 10\n",
    "Child.extra = 20\n",
    "\n",
    "\n",
    "c=Child(10,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor 张量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pytorch, the core data type is tensor, which is a multi-dimensional matrix containing elements of a single data type. The core operation associated with this data type is gradient calculation. In this notebook, we will introduce some basic concepts and code operations related to it.\n",
    "\n",
    "在PyTorch框架中的一个核心数据类型是张量，即Tensor。伴随此数据类型的核心操作是梯度计算。下面将逐一说明与之相关一些基本概念和代码操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Creating Tensor 张量的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "t0 = torch.zeros(5, 3, 2)                          #各元素用0填充\n",
    "t1 = torch.ones(5, 3, 2)                           #各元素用1填充\n",
    "tr = torch.randn(5, 3, 2)                          #各元素用标准正态分布随机数填充\n",
    "lis2t = torch.tensor([[1,2],[3,4],[5,6]])          # 直接由列表数据构造张量\n",
    "ta = torch.arange(12.0)                            # 由浮点型序列构造张量\n",
    "arr2t = torch.tensor(np.array([[1,2],[3,4]]))      # 将np数组转化成张量\n",
    "mat2t = torch.tensor(np.matrix([[1,3],[3,5]]))     # 将np矩阵转换成张量\n",
    "t2mat = np.matrix(mat2t)                           # 将张量转换成np矩阵\n",
    "t2arr = np.array(arr2t)                            # 将张量转换成np数组\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tensor Attributes 张量的属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "s0 = t0.shape                                      # 张量的形状\n",
    "device0 = t0.device                                # 张量的运算设备（如GPU或CPU）\n",
    "print(t0.requires_grad)                            # 当前张量是否会被视为自变量来计算梯度\n",
    "print(t0.is_leaf)                                  # 当前张量是否为叶子节点（即手创或由“不计算梯度”张量运算出的张量）\n",
    "print(t0.grad_fn)                                  # 生成当前张量的生成函数（凡叶子节点的生成函数都为None）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tensor Operations 张量的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ele = tr[0][1][1].item()                           # 取张量中的一个元素。注意：只有标量才能取其值\n",
    "tta = ta.reshape(2, 2, 3)                          # 更改张量形状，但元素数要与原张量一致\n",
    "arr2ft = arr2t.float()                             # 将张量中各元素类型转为浮点型，许多张量运算只适用于浮点型\n",
    "y1 = arr2ft * mat2t                                # 得到两张量的对应元素乘积\n",
    "y2 = arr2ft @ arr2ft                               # 对2阶张量（矩阵）做点积\n",
    "y3 = torch.t(arr2t)                                # 对2阶以下张量求其转置\n",
    "y4 = torch.inverse(arr2ft)                         # 对2阶张量求其逆张量（张量中元素必须是浮点型）\n",
    "m2 = y2.mean()                                     # 求张量中各元素均值（也只适用于浮点型元素）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tensor Gradient 张量的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<MmBackward0 object at 0x0000027A524BDB70>\n",
      "None\n",
      "tensor([[-0.3750,  0.3750],\n",
      "        [-0.3750,  0.3750]])\n"
     ]
    }
   ],
   "source": [
    "arr2ft.requires_grad = True                        # 将要计算梯度的张量的“计算梯度”属性置为真，默认是假\n",
    "yy = arr2ft @ arr2ft                               # 做矩阵乘法\n",
    "yy.requires_grad                                   # “计算梯度”属性置真后算得的张量，其该属性也自动为真\n",
    "print(yy.is_leaf)                                  # 由“计算梯度”张量运算出的新张量不再是叶子节点\n",
    "print(arr2ft.is_leaf)                              # 手创的“计算梯度”张量是叶子节点\n",
    "print(yy.grad_fn)                                  # 非叶子张量会有生成函数\n",
    "fy = yy.mean()                                     # 由张量运算出标量，因为只能针对作为标量的最终函数值计算梯度\n",
    "print(arr2ft.grad)                                 # 梯度计算前叶子张量的梯度属性为空\n",
    "fy.backward()                                      # 利用偏导的反向传递计算各叶子张量的梯度\n",
    "print(arr2ft.grad)                                 # 每次算得的梯度会累加进叶子张量的梯度属性里\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch framework, no matter how complex the function operation is, whether it is a library function or a handwritten one, as long as a scalar function value can be obtained in the end, the gradient of this function value to each leaf tensor can be traced back. The mathematical principle behind it is the partial derivative calculation formula of multivariate functions.\n",
    "\n",
    "在PyTorch框架中，各叶子张量无论经历怎样复杂的函数运算，这些函数也无论是库函数还是手写的，只要最终能得到一个标量函数值，都能追溯到此函数值对各叶子张量的梯度，其背后的数学原理就是多元函数的偏导计算公式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 更新叶子张量后使其仍保持叶子属性\n",
    "\n",
    "前面提到叶子节点必须是手创或是全由\"不计算梯度\"的张量运算出的张量，但在神经网络训练过程中，待训练的参数张量一方面必须要设置成是“计算梯度”的，另一方面在每次训练中要被迭代更新，而这更新过程就是由原参数张量参与计算得到新参数张量，这样一来，新参数张量必然不再是叶子节点，也就无法再进行下一次训练迭代了。为解决此问题，需要设法使更新后的叶子张量仍保持其叶子属性。此时要用到Python的上下文操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'Tensor' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m arr2ft\u001b[38;5;241m.\u001b[39mrequires_grad                                \u001b[38;5;66;03m# 梯度计算开关也置回默认关闭状态\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(arr2ft\u001b[38;5;241m.\u001b[39mgrad)                                  \u001b[38;5;66;03m# 梯度属性也自动清空\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m arr2ft \u001b[38;5;241m=\u001b[39m \u001b[43marr2ft\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43marr2ft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m                       \u001b[38;5;66;03m# 如果直接更新叶子张量，则会将其变为生成张量，即非叶子张量\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(arr2ft\u001b[38;5;241m.\u001b[39mis_leaf)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'Tensor' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    arr2ft = arr2ft - arr2ft.grad                   # 在no_grad上下文中更新叶子张量\n",
    "print(arr2ft.is_leaf)                               # 此时原叶子张量虽被运算更新，但仍恢复成最初叶子属性\n",
    "arr2ft.requires_grad                                # 梯度计算开关也置回默认关闭状态\n",
    "print(arr2ft.grad)                                  # 梯度属性也自动清空\n",
    "arr2ft = arr2ft - arr2ft.grad                       # 如果直接更新叶子张量，则会将其变为生成张量，即非叶子张量\n",
    "print(arr2ft.is_leaf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every layer is nn.Module. It is a base class for all neural network modules. Your models should also subclass this class.\n",
    "\n",
    "每一个层都是nn.Module。它是所有神经网络模块的基类。你的模型也应该是这个类的子类。\n",
    "\n",
    "And nn.Module is nested, which means you can put an nn.Module inside another nn.Module.\n",
    "\n",
    "并且nn.Module是嵌套的，这意味着你可以把一个nn.Module放在另一个nn.Module里。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 nn.Module提供的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nn.Module提供了一些方法，比如：\n",
    "\"\"\"\n",
    "nn.Linear() #线性变换\n",
    "nn.Conv2d() #二维卷积\n",
    "nn.BatchNorm2d() #批标准化\n",
    "nn.ReLU() #激活函数\n",
    "nn.Sigmoid() #激活函数\n",
    "nn.MaxPool2d() #最大池化\n",
    "nn.CrossEntropyLoss() #交叉熵损失\n",
    "nn.MSELoss() #均方误差损失\n",
    "nn.Sequential() #序列容器\n",
    "nn.ModuleList() #模块列表\n",
    "nn.ModuleDict() #模块字典\n",
    "nn.functional #常用函数库\n",
    "nn.Dropout() #Dropout层\n",
    "nn.ConvTranspose1d()  #一维转置卷积\n",
    "\"\"\"\n",
    "#等等\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.Sequential的作用是将网络的层组合到一起，以形成一个新的网络。\n",
    "\n",
    "这样可以简化网络的结构，使得网络的结构更加清晰，不必在forward函数中一层一层的写网络结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对比以下代码，可以看出nn.Module的优势\n",
    "\n",
    "# 1. 无nn.Module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)  # 直接使用functional API调用ReLU\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)  # 再次使用functional API调用ReLU\n",
    "        return x\n",
    "    \n",
    "\n",
    "# 2. 有nn.Module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        x = self.fc2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推荐使用第一种方法，即将激活函数作为模块来使用。这种方式使得模型的结构更加清晰，并且更容易进行模块的重用和管理。直接调用激活函数（如F.relu）虽然在编写代码时可能更快，但在处理复杂模型时可能会减少代码的可读性和可维护性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对比以下代码，可以看出nn.sequential的优势\n",
    "\n",
    "# 1. 无nn.sequential\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #需要给每个层命名\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5)\n",
    "        self.conv2 = nn.Conv2d(20, 64, 5)\n",
    "        self.fc1 = nn.Linear(64 * 5 * 5, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "# 2. 有nn.sequential\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #不需要给每个层命名\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 20, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(20, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64 * 5 * 5, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64 * 5 * 5)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过net.parameters()方法可以查看模型的参数\n",
    "net = Net()\n",
    "for param in net.parameters():\n",
    "    print(param)\n",
    "# 通过net.named_parameters()方法可以查看模型的参数及对应的名字\n",
    "for name, param in net.named_parameters():\n",
    "    print(name, param.size())\n",
    "\n",
    "# 通过list()方法可以查看模型的参数\n",
    "print(list(net.parameters()))\n",
    "# 通过dict()方法可以查看模型的参数及对应的名字\n",
    "print(dict(net.named_parameters()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 模块的节点图（子节点） Module Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 通过net.modules()方法可以查看模型的所有模块\n",
    "for module in net.modules():\n",
    "    print(module)\n",
    "# 通过net.named_modules()方法可以查看模型的所有模块及对应的名字\n",
    "for name, module in net.named_modules():\n",
    "    print(name, module)\n",
    "# 通过net.children()方法可以查看模型的所有子模块\n",
    "for child in net.children():\n",
    "    print(child)\n",
    "# 通过net.named_children()方法可以查看模型的所有子模块及对应的名字\n",
    "for name, child in net.named_children():\n",
    "    print(name, child)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 模式切换以及模型保存和加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 通过net.train()方法可以将模型设置为训练模式\n",
    "net.train()\n",
    "# 通过net.eval()方法可以将模型设置为评估模式\n",
    "net.eval()\n",
    "# 通过net.to()方法可以将模型转移到指定的设备\n",
    "net.to('cuda:0')\n",
    "# 通过net.apply()方法可以对模型的所有模块进行指定的操作\n",
    "def init_weights(m):\n",
    "    print(m)\n",
    "net.apply(init_weights)\n",
    "# 通过net.zero_grad()方法可以将模型的参数的梯度清零\n",
    "net.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 通过net.state_dict()方法可以查看模型的状态字典\n",
    "print(net.state_dict())\n",
    "# 通过net.load_state_dict()方法可以加载模型的状态字典\n",
    "net.load_state_dict(torch.load('model.pth'))\n",
    "# 通过net.save()方法可以保存模型的状态字典\n",
    "torch.save(net.state_dict(), 'model.pth')\n",
    "# 通过net.load()方法可以加载模型的状态字典\n",
    "net.load_state_dict(torch.load('model.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
