{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function/Emprical Risk Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is the cornerstone of the empirical risk function. The **empirical risk function** is essentially the average of the loss function across all training samples; it is the accumulation of the loss function over the training dataset. The training process in machine learning typically involves minimizing the empirical risk function, which means finding a set of model parameters $\\theta$ that minimizes the average loss on the training data.\n",
    "\n",
    "损失函数是构成经验风险函数的基石。经验风险函数实际上是损失函数在所有训练样本上的平均值，是损失函数在训练数据集上的累积。机器学习中的训练过程通常涉及最小化经验风险函数，这意味着找到一组模型参数$\\theta$，使得在训练数据上的平均损失最小。\n",
    "\n",
    "\n",
    "In short, the loss function is a measure of the prediction error for a single sample, while the empirical risk function is the average outcome of applying the loss function across the entire dataset. Empirical risk minimization is the process of choosing model parameters to minimize the average loss on the training set.\n",
    "\n",
    "简而言之，损失函数是对单个样本预测错误的度量，而经验风险函数是在整个数据集上应用损失函数后的平均结果。经验风险最小化是选择模型参数以最小化训练集上的平均损失的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss function or cost function (sometimes also called an error function)  is a function that maps an event or values of one or more variables onto a real number intuitively representing some \"cost\" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.\n",
    "\n",
    "损失函数（loss function）是用来估量模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。模型的结构风险函数包括了经验风险项和正则项，通常可以表示成如下式子。\n",
    "\n",
    "$$\\theta^* = \\arg\\min_{\\theta} \\left[ \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, f(x_i; \\theta)) + \\lambda \\Phi(\\theta) \\right]$$\n",
    "\n",
    "The loss function is a part of the objective function, which can be a combination of loss function and regularization term. The regularization term is used to avoid overfitting by adding a penalty for complexity. The objective function is then used to train the model by adjusting the model parameters.\n",
    "\n",
    "\n",
    "The mean function in the front represents the empirical risk function, and L stands for the loss function. The Φ at the back is the regularization term (regularizer) or penalty term, which can be L1, L2, or other regularization functions. The entire expression means finding the θ value that minimizes the objective function.\n",
    "\n",
    "其中，前面的均值函数表示的是经验风险函数，L代表的是损失函数，后面的Φ是正则化项（regularizer）或者叫惩罚项（penalty term），它可以是L1，也可以是L2，或者其他的正则函数。整个式子表示的意思是找到使目标函数最小时的θ值。\n",
    "\n",
    "一般情况下，softmax和sigmoid使用交叉熵损失（logloss），hingeloss是SVM推导出的，hingeloss的输入使用原始logit即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LogLoss Function 对数损失函数(Logistic Loss & Cross-Entropy Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function would log every prediction and true value, which is usually used in classification problems, because its value ranges from 0 to 1, and we hope that the larger the difference between the predicted value and the true value, the larger the value of the loss function.\n",
    "\n",
    "将每一组预测值与真实值取对数，这一行为通常用于分类问题，因为其值的范围在0-1之间，而我们希望在预测值与真实值相差越大时，对应的损失函数值越大。\n",
    "\n",
    "There are two types of logloss functions: logistic loss and cross-entropy loss. The logistic loss is used for binary classification, and the cross-entropy loss is used for multi-class classification.\n",
    "\n",
    "There are some wrong idea about logloss function. Some people may think that the loss function of logistic regression is the square loss, but it is not. The square loss function can be derived from linear regression under the assumption that the sample is a Gaussian distribution, while the logistic regression does not obtain the square loss. \n",
    "\n",
    "有些人可能觉得逻辑回归的损失函数就是平方损失，其实并不是。平方损失函数可以通过线性回归在假设样本是高斯分布的条件下推导得到，而逻辑回归得到的并不是平方损失。\n",
    "\n",
    "In the derivation of logistic regression, it assumes that the sample follows the Bernoulli distribution (0-1 distribution), and then obtains the likelihood function that satisfies the distribution, and then takes the logarithm to find the extremum, etc. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "在逻辑回归的推导中，它假设样本服从伯努利分布（0-1分布），然后求得满足该分布的似然函数，接着取对数求极值等等。\n",
    "\n",
    "Logistic regression does not find the extremum of the likelihood function, but treats it as a kind of thought, and then derives its empirical risk function: minimizing the negative likelihood function (i.e. max F(y, f(x)) —> min -F(y, f(x))). From the perspective of the loss function, it becomes a log loss function.\n",
    "\n",
    "而逻辑回归并没有求似然函数的极值，而是把极大化当做是一种思想，进而推导出它的经验风险函数为：最小化负的似然函数（即max F(y, f(x)) —> min -F(y, f(x)))。从损失函数的视角来看，它就成了log损失函数了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "L(Y, P(Y|X)) = -\\log P(Y|X)\n",
    "\\tag{1}\n",
    "\\end{equation}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the logarithm is to facilitate the calculation of the maximum likelihood estimation (MLE), because it is difficult to directly take the derivative in MLE, so it is usually to take the logarithm first and then take the derivative to find the extreme value.\n",
    "\n",
    "取对数是为了方便计算极大似然估计，因为在MLE（最大似然估计）中，直接求导比较困难，所以通常都是先取对数再求导找极值点。\n",
    "\n",
    "Loss Function L(Y, P(Y|X)) expresses that when the sample X is classified as Y, the probability P(Y|X) is maximized (in other words, it is to find the parameter value that is most likely to lead to this distribution using the known sample distribution; or what parameters can make the probability of observing this group of data the largest).\n",
    "\n",
    "损失函数L(Y, P(Y|X))表达的是样本X在分类Y的情况下，使概率P(Y|X)达到最大值（换言之，就是利用已知的样本分布，找到最有可能（即最大概率）导致这种分布的参数值；或者说什么样的参数才能使我们观测到目前这组数据的概率最大）。\n",
    "\n",
    "Because the log function is monotonically increasing, logP(Y|X) will also reach the maximum value, so adding a negative sign in front will make the maximum P(Y|X) equivalent to minimizing L.\n",
    "\n",
    "因为log函数是单调递增的，所以logP(Y|X)也会达到最大值，因此在前面加上负号之后，最大化P(Y|X)就等价于最小化L了。\n",
    "\n",
    "The logistic regression P(Y=y|x) expression is as follows (to unify the category label y as 1 and 0, the expression is separated as follows)\n",
    "\n",
    "逻辑回归的P(Y=y|x)表达式如下（为了将类别标签y统一为1和0，下面将表达式分开表示）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{equation}\n",
    "P(Y = y|x) = \n",
    "\\begin{cases} \n",
    "h_\\theta(x) = g(f(x)) = \\frac{1}{1 + \\exp(-f(x))}, & \\text{for } y = 1 \\\\\n",
    "1 - h_\\theta(x) = 1 - g(f(x)) = \\frac{1}{1 + \\exp(f(x))}, & \\text{for } y = 0\n",
    "\\end{cases}\n",
    "\\tag{2}\n",
    "\\end{equation}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代入上式，得到逻辑损失函数表达式：\n",
    "\n",
    "$$\\begin{equation}\n",
    "L(y, P(Y = y|x)) = \n",
    "\\begin{cases} \n",
    "\\log(1 + \\exp(-f(x))), & \\text{for } y = 1 \\\\\n",
    "\\log(1 + \\exp(f(x))), & \\text{for } y = 0\n",
    "\\end{cases}\n",
    "\\tag{3}\n",
    "\\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic loss function is the sum of the two cases, and the final expression is as follows:\n",
    "\n",
    "逻辑回归最后得到的目标式子如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log h_{\\theta}(x^{(i)}) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]\n",
    "\\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula above pertains specifically to binary classification.\n",
    "\n",
    "上面是针对二分类而言的。\n",
    "\n",
    "**Note:** The reason some might consider logistic regression to have a quadratic loss is because, when using gradient descent to find the optimal solution, its iteration formula appears very similar to the derivative of the quadratic loss function. This similarity can intuitively give the misleading impression that logistic regression operates with a quadratic loss.\n",
    "\n",
    "**注意：**之所以有人认为逻辑回归是平方损失，是因为在使用梯度下降来求最优解的时候，它的迭代式子与平方损失求导后的式子非常相似，从而给人一种直观上的错觉。\n",
    "\n",
    "The corresponding loss function for softmax is the cross-entropy loss. \n",
    "\n",
    "softmax对应使用的即为交叉熵损失函数,\n",
    "\n",
    "For binary classification, the binary_crossentropy loss is used\n",
    "\n",
    "And for multi-class classification, the categorical_crossentropy loss is employed. \n",
    "\n",
    "When utilizing the multi-class cross-entropy loss function, the labels should be in multi-class format, that is, vectors that are one-hot encoded.\n",
    "\n",
    "当使用多分类交叉熵损失函数时，标签应该为多分类模式，即使用one-hot编码的向量。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. SquareLoss Function 平方损失函数(最小二乘法, Ordinary Least Squares )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Squares (OLS) is a type of linear regression that transforms the problem into a [convex optimization problem](https://en.wikipedia.org/wiki/Convex_optimization).\n",
    "\n",
    "\n",
    "最小二乘法是线性回归的一种，最小二乘法（OLS）将问题转化成了一个[凸优化问题](https://zh.wikipedia.org/wiki/%E5%87%B8%E5%84%AA%E5%8C%96)。\n",
    "\n",
    "In linear regression, it is assumed that both the samples and the noise follow a Gaussian distribution (Why a Gaussian distribution? Actually, this assumption involves about the [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)). Ultimately, through Maximum Likelihood Estimation (MLE), the least squares formula can be derived. \n",
    "\n",
    "在线性回归中，它假设样本和噪声都服从高斯分布（为什么假设成高斯分布呢？其实这里隐藏了一个小点，就是[中心极限定理](https://zh.wikipedia.org/wiki/%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86)），最后通过极大似然估计（MLE）可以推导出最小二乘式子。\n",
    "\n",
    "The fundamental principle of least squares is that the optimal fitting line should be the line that minimizes the sum of the distances from the points to the regression line, that is, the sum of the squares is minimized.\n",
    "\n",
    "最小二乘的基本原则是：最优拟合直线应该是使各点到回归直线的距离和最小的直线，即平方和最小。\n",
    "\n",
    "In other words, OLS is based on distance, and this distance is the Euclidean distance we most commonly use. Why choose Euclidean distance as the error metric (i.e., Mean Squared Error, MSE)? There are several main reasons:\n",
    "\n",
    "换言之，OLS是基于距离的，而这个距离就是我们用的最多的欧几里得距离。为什么选择使用欧式距离作为误差度量呢（即Mean squared error， MSE），主要有以下几个原因：\n",
    "\n",
    "\n",
    "- It is simple and computationally convenient;\n",
    "  - 简单，计算方便；\n",
    "- Euclidean distance is a very good measure of similarity;\n",
    "  - 欧氏距离是一种很好的相似性度量标准；\n",
    "- The characteristic properties remain invariant under different representational domain transformations.\n",
    "  - 在不同的表示域变换后特征性质不变。\n",
    "\n",
    "The Square Loss function is as follows:\n",
    "\n",
    "平方损失（Square loss）的标准形式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "L(Y, f(X)) = (Y - f(X))^2\n",
    "\\tag{5}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{equation}\n",
    "L(Y, f(X)) = \\sum_{i=1}^{n} (Y - f(X_i))^2\n",
    "\\tag{6}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire expression represents the sum of the squares of the residuals, and our goal is to minimize the value of this objective function (Note: the expression does not include a regularization term), which means minimizing the residual sum of squares (RSS).\n",
    "\n",
    "整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和（residual sum of squares，RSS）。\n",
    "\n",
    "In practical applications, the Mean Squared Error (MSE) is commonly used as a metric for measurement, the formula for which is as follows:\n",
    "\n",
    "而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Y}_i - Y_i)^2\n",
    "\\tag{7}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Adaboost Function 指数损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaboost is a special case of the forward stepwise addition algorithm, and it is an additive model. The loss function is the exponential function. After m iterations, the fm(x) can be obtained in Adaboost:\n",
    "\n",
    "Adaboost算法是前向分步加法算法的特例，是一个加和模型，损失函数就是指数函数。在Adaboost中，经过m此迭代之后，可以得到fm(x):\n",
    "\n",
    "$$ \n",
    "\\begin{equation}\n",
    "f_m(x) = f_{m-1}(x) + \\alpha_m G_m(x)\n",
    "\\tag{8}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Adaboost's every iteration is to find the parameters ${\\alpha}$ and G that minimize the following expression:\n",
    "\n",
    "Adaboost每次迭代时的目的是为了找到最小化下列式子时的参数 ${\\alpha}$ 和G：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\arg\\min_{\\alpha, G} \\sum_{i=1}^{N} \\exp\\left[-y_i \\left(f_{m-1}(x_i) + \\alpha G(x_i)\\right)\\right]\n",
    "\\tag{9}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "And the loss function is the exponential loss function, whose standard form is as follows:\n",
    "\n",
    "而指数损失函数（exp-loss）的标准形式如下:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(y, f(x)) = \\exp[-yf(x)]\n",
    "\\tag{10}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Adaboost's objective function is the exponential loss, and in the case of n samples, the Adaboost loss function is:\n",
    "\n",
    "Adaboost的目标式子就是指数损失，在给定n个样本的情况下，Adaboost的损失函数为：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(y, f(x)) = -\\frac{1}{n} \\sum_{i=1}^{n} \\exp[-y_i f(x_i)]\n",
    "\\tag{11}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. HingeLoss Function 合页损失函数(SVM支持向量机)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hinge loss is a loss function used for training classifiers. The hinge loss is used for \"maximum-margin\" classification, most notably for support vector machines (SVMs). In SVMs, the most oprimalization problem can be equivalent to the following expression:\n",
    "\n",
    "hinge损失函数和SVM是息息相关的。在线性支持向量机中，最优化问题可以等价于下列式子：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\min_{w,b} \\sum_{i=1}^{N} [1 - y_i(w \\cdot x_i + b)]_+ + \\lambda \\|w\\|^2\n",
    "\\tag{12}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Transform the equation above, let:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "[1 - y_i(w \\cdot x_i + b)]_+ = \\xi_i\n",
    "\\tag{13}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In above equation, the ${\\xi_i}$ is the slack variables(松弛变量), aiming to deal with the situation that the data is not linearly separable(线性不完全可分). Therefore, the origin equation turns to:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\min_{w,b} \\sum_{i=1}^{N} \\xi_i + \\lambda \\|w\\|^2\n",
    "\\tag{14}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If the ${\\lambda}=\\frac{1}{2C}$, the equation can be expressed as:\n",
    "\n",
    "如若取λ=1/(2C)，式子就可以表示成：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\min_{w,b} \\frac{1}{C} \\left( \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} \\xi_i \\right)\n",
    "\\tag{15}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Which is similar with the following expression:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{1}{m} \\sum_{i=1}^{m} l(w \\cdot x_i + b, y_i) + \\lambda \\|w\\|^2\n",
    "\\tag{16}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The former part in the above expression is the hinge loss function, and the latter part is the L2 regularization term.\n",
    "\n",
    "前半部分中的就是hinge损失函数，而后面相当于L2正则项。\n",
    "\n",
    "And there is the standard form of the hinge loss function:\n",
    "\n",
    "这就是hinge损失函数的标准形式：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(y) = \\max(0, 1 - yy')\n",
    "\\tag{17}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. HuberLoss Function Huber损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huber Loss Function is a robust and sensitive loss function that is often used for regression problems. The Huber loss solves the problem of the square loss function being too sensitive to outliers, while retaining the sensitivity of the square loss to errors in the central region.\n",
    "\n",
    "Huber Loss Function，它是一种既鲁棒又兼具平方误差敏感性的损失函数，通常用于回归问题。Huber loss解决了平方损失函数对于离群点（outliers）过于敏感的问题，同时保留了平方损失在中心区域对误差敏感的特点。\n",
    "\n",
    "The Huber loss function is defined as follows:\n",
    "\n",
    "Huber损失函数的定义如下：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Other Loss Function 其他损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  0-1损失函数(0-1 Loss) & 绝对值损失函数(Absolute Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-1Loss Function is a loss function that is 1 when the prediction is wrong and 0 when the prediction is correct. The 0-1 loss function is a non-convex function, so it is not suitable for use in optimization algorithms.\n",
    "\n",
    "0-1损失函数是指当预测值和真实值不相等时，损失为1，否则为0。0-1损失函数是一个非凸函数，因此不适合在优化算法中使用。\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(Y, f(X)) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } Y \\neq f(X) \\\\\n",
    "0, & \\text{if } Y = f(X)\n",
    "\\end{cases}\n",
    "\\tag{19}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Absolute Loss Function is a loss function that is the absolute value of the difference between the prediction and the true value. The absolute loss function is also a non-convex function, so it is not suitable for use in optimization algorithms.\n",
    "\n",
    "绝对值损失函数是指损失函数为预测值和真实值之差的绝对值。同样的，绝对值损失函数也是一个非凸函数，因此不适合在优化算法中使用。\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "L(Y, f(X)) = |Y - f(X)|\n",
    "\\tag{20}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all the graphs of every loss functions:\n",
    "\n",
    "![](https://ask.qcloudimg.com/http-save/yehe-1881084/nkhl6fmtst.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "**Some common loss functions in Keras/Tensorflow are as follows:**\n",
    "\n",
    "- ***Regression Loss Function***\n",
    "    - **mean_absolute_error(MAE)**\n",
    "    - **mean_absolute_percentage_error(MAPE)**\n",
    "    - **mean_squared_error(MSE)**\n",
    "    - **mean_squared_logarithmic_error(MSLE)**\n",
    "- ***Classification Loss Function***\n",
    "  - ***hinge***\n",
    "    - **squared_hinge**\n",
    "    - **categorical_hinge**\n",
    "  - ***CrossEntropy***\n",
    "    - **binary_crossentropy**（亦称作对数损失，logloss）\n",
    "    - **kullback_leibler_divergence**:从预测值概率分布Q到真值概率分布P的信息增益,用以度量两个分布的差异.\n",
    "    - **poisson**：即(predictions - targets * log(predictions))的均值\n",
    "    - **cosine_proximity**：即预测值与真实标签的余弦距离平均值的相反数\n",
    "    - **sparse_categorical_crossentrop**：接受稀疏标签。注意，使用该函数时仍然需要你的标签与输出值的维度相同，你可能需要在标签数据上增加一个维度：np.expand_dims(y,-1)\n",
    "    - **categorical_crossentropy**：亦称作多类的对数损失，注意使用该目标函数时，需要将标签转化为形如(nb_samples, nb_classes)的二值序列\n",
    "    - **logcosh**: Logarithm of the hyperbolic cosine of the prediction error. 对数双曲余弦损失函数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "\n",
    "Generally, the choice of loss function depends on the specific problem and the characteristics of the data. \n",
    "\n",
    "More parameters means a more complex model, and the more complex the model, the easier it is to overfit. Overfitting means that the model performs much better on the training data than on the test set. \n",
    "\n",
    "参数越多，模型越复杂，而越复杂的模型越容易过拟合。过拟合就是说模型在训练数据上的效果远远好于在测试集上的性能。\n",
    "\n",
    "At this time, regularization can be considered, by setting the hyper parameter in front of the regularization term to balance the loss function and the regularization term, reduce the parameter scale, simplify the model, and improve the generalization ability of the model.\n",
    "\n",
    "此时可以考虑正则化，通过设置正则项前面的hyper parameter，来权衡损失函数和正则项，减小参数规模，达到模型简化的目的，从而使模型具有更好的泛化能力。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
