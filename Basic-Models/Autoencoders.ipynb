{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder 自动编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1 What is Autoencoder?\n",
    "\n",
    "Autoencoder is a type of artificial neural network used in unsupervised learning and semi-supervised learning. Its function is to learn the representation of input information by using the input information as the learning target.\n",
    "\n",
    "自编码器（autoencoder, AE）是一类在半监督学习和非监督学习中使用的人工神经网络（Artificial Neural Networks, ANNs），其功能是通过将输入信息作为学习目标，对输入信息进行表征学习（representation learning）\n",
    "\n",
    "### 1.2 Structure of Autoencoder\n",
    "\n",
    "The Autoencoder consists of two parts: the encoder and the decoder. The encoder is used to map the input information to the hidden layer, and the decoder is used to map the hidden layer to the output layer. The hidden layer is the representation of the input information.\n",
    "\n",
    "包含编码器（encoder）和解码器（decoder）两部分。编码器用于将输入信息映射到隐藏层，解码器用于将隐藏层映射到输出层。隐藏层是输入信息的表征。\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20190531172354847.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hfX2FuZw==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "From above figure, we can see two parts: the first part is the encoder, the second part is the decoder. \n",
    "\n",
    "从上图可以看到两个部分：第一个部分是编码器（Encoder），第二个部分是解码器（Decoder），\n",
    "\n",
    "The encoder and decoder can be any model, usually using neural networks as the encoder and decoder. The input data is reduced to a code through a neural network, and then another neural network is used to decode it to get a generated data that is exactly the same as the input original data. \n",
    "\n",
    "编码器和解码器都可以是任意的模型，通常使用神经网络作为编码器和解码器。输入的数据经过神经网络降维到一个编码（code），接着又通过另外一个神经网络去解码得到一个与输入原数据一模一样的生成数据，\n",
    "\n",
    "Then by comparing these two data, the parameters of the encoder and decoder in this network are trained by minimizing the difference between them.\n",
    "\n",
    "然后通过比较这两个数据，最小化它们之间的差异来训练这个网络中编码器和解码器的参数。\n",
    "\n",
    "Based on the learning paradigm, the Autoencoder can be divided into three types: the Contractive Autoencoder, the Regularized Autoencoder, and the Variational Autoencoder. The first two are discriminative models, and the latter is a generative model. The Autoencoder can be a feedforward neural network or a recursive neural network.\n",
    "\n",
    "按学习范式，自编码器可以被分为收缩自编码器（contractive autoencoder）、正则自编码器（regularized autoencoder）和变分自编码器（Variational AutoEncoder, VAE），其中前两者是判别模型、后者是生成模型。按构筑类型，自编码器可以是前馈结构或递归结构的神经网络。\n",
    "\n",
    "### 1.3 Application of Autoencoder\n",
    "\n",
    "Autoencoder has the function of representation learning, which is applied to dimensionality reduction and anomaly detection. Autoencoder constructed with convolutional layers can be applied to computer vision problems, including image denoising, neural style transfer, etc.\n",
    "\n",
    "自编码器具有一般意义上表征学习算法的功能，被应用于降维（dimensionality reduction）和异常值检测（anomaly detection）。包含卷积层构筑的自编码器可被应用于计算机视觉问题，包括图像降噪（image denoising）、神经风格迁移（neural style transfer）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autoencoder Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将input输入一个encoder编码器，就会得到一个code，这个code也就是输入的一个表示，然后加一个decoder解码器，这时候decoder就会输出一个信息，如果输出的这个信息和一开始的输入信号input是很像的（理想情况下就是一样的），那很明显，我们就有理由相信这个code是靠谱的。所以，我们就通过调整encoder和decoder的参数，使得重构误差最小，这时候我们就得到了输入input信号的第一个表示了，也就是编码code了。因为是无标签数据，所以误差的来源就是直接重构后与原输入相比得到。我们的重构误差最小让我们相信这个code就是原输入信号的良好表达了，或者牵强点说，它和原信号是一模一样的。接着，我们将第一层输出的code当成第二层的输入信号，同样最小化重构误差，就会得到第二层的参数，并且得到第二层输出的code，也就是原输入信息的第二个表达了。其他层就同样的方法炮制就行了（训练这一层，前面层的参数都是固定的，并且他们的decoder已经没用了，都不需要了）。\n",
    "\n",
    "\n",
    "需要注意的是，整个网络的训练不是一蹴而就的，而是逐层进行。如果按n→m→k\n",
    "结构，实际上我们是先训练网络n→m→n，得到n→m的变换，然后再训练m→k→m，得到m→k的变换。最终堆叠成SAE，即为n→m→k的结果，整个过程就像一层层往上盖房子，这便是大名鼎鼎的 layer-wise unsuperwised pre-training （逐层非监督预训练），正是导致深度学习（神经网络）在2006年第3次兴起的核心技术。\n",
    "\n",
    "\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20190531174754287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2hfX2FuZw==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "若给code加入一定的约束。从数据维度来看，常见以下两种情况：\n",
    "\n",
    "If the dimension of input is greater than the dimension of code, that is, the transformation from $input\\rightarrow code$ is a dimensionality reduction operation, the network tries to describe the original data with a smaller dimension without losing data information. In fact, when the transformation between two layers is linear and the loss function is the mean square error loss function, the network is equivalent to PCA (Principal Component Analysis);\n",
    "\n",
    "如果input的维度大于code的维度，也就是说从$input\\rightarrow code$的变换是一种降维的操作，网络试图以更小的维度来描述原始数据而尽量不损失数据信息。实际上，当两层之间的变换均为线性，且损失函数为平方差损失函数时，该网络等价于PCA(主成分分析)；\n",
    "\n",
    "\n",
    "如果input的维度小于等于code的维度。这又有什么用呢？其实不好说，但比如我们同时约束code的表达尽量稀疏（有大量维度为0，未被激活），此时的编码器便是大名鼎鼎的“稀疏自编码器”。可为什么稀疏的表达就是好的呢？这就说来话长了，有人试图从人脑机理对比，即人类神经系统在某刺激下，大部分神经元是被抑制的。个人觉得，从特征的角度来看直接些，稀疏的表达意味着系统在尝试去特征选择，找出大量维度中真正重要的若干维度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Autoencoder Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 基于全连接网络的自编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据集\n",
    "def get_data():\n",
    "    # 将像素点转换到[-1, 1]之间，使得输入变成一个比较对称的分布，训练容易收敛\n",
    "    data_tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=data_tf, download=True)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    return train_loader\n",
    "\n",
    "def to_img(x):\n",
    "    x = (x + 1.) * 0.5\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(28*28, 128),\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.Linear(128, 64),\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.Linear(64, 12),\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(nn.Linear(3, 12),\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.Linear(12, 64),\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.Linear(64, 128),\n",
    "                                     nn.ReLU(True),\n",
    "                                     nn.Linear(128, 28*28),\n",
    "                                     nn.Tanh())\n",
    "    def forward(self, x):\n",
    "        encode = self.encoder(x)\n",
    "        decode = self.decoder(encode)\n",
    "        return encode, decode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 tensor(0.2223, device='cuda:0')\n",
      "0.01\n",
      "epoch= 1 tensor(0.2091, device='cuda:0')\n",
      "0.01\n",
      "epoch= 2 tensor(0.2007, device='cuda:0')\n",
      "0.01\n",
      "epoch= 3 tensor(0.1928, device='cuda:0')\n",
      "0.01\n",
      "epoch= 4 tensor(0.2042, device='cuda:0')\n",
      "0.01\n",
      "epoch: 5, loss is 0.2042459398508072\n",
      "epoch= 5 tensor(0.2037, device='cuda:0')\n",
      "0.01\n",
      "epoch= 6 tensor(0.2123, device='cuda:0')\n",
      "0.01\n",
      "epoch= 7 tensor(0.1972, device='cuda:0')\n",
      "0.01\n",
      "epoch= 8 tensor(0.1886, device='cuda:0')\n",
      "0.01\n",
      "epoch= 9 tensor(0.1810, device='cuda:0')\n",
      "0.01\n",
      "epoch: 10, loss is 0.18100722134113312\n",
      "epoch= 10 tensor(0.1785, device='cuda:0')\n",
      "0.001\n",
      "epoch= 11 tensor(0.1804, device='cuda:0')\n",
      "0.001\n",
      "epoch= 12 tensor(0.1749, device='cuda:0')\n",
      "0.001\n",
      "epoch= 13 tensor(0.1841, device='cuda:0')\n",
      "0.001\n",
      "epoch= 14 tensor(0.1842, device='cuda:0')\n",
      "0.001\n",
      "epoch: 15, loss is 0.18416298925876617\n",
      "epoch= 15 tensor(0.1728, device='cuda:0')\n",
      "0.001\n",
      "epoch= 16 tensor(0.1743, device='cuda:0')\n",
      "0.001\n",
      "epoch= 17 tensor(0.1727, device='cuda:0')\n",
      "0.001\n",
      "epoch= 18 tensor(0.1797, device='cuda:0')\n",
      "0.001\n",
      "epoch= 19 tensor(0.1734, device='cuda:0')\n",
      "0.001\n",
      "epoch: 20, loss is 0.17343050241470337\n",
      "epoch= 20 tensor(0.1678, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 21 tensor(0.1691, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 22 tensor(0.1598, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 23 tensor(0.1637, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 24 tensor(0.1669, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 25, loss is 0.16685990989208221\n",
      "epoch= 25 tensor(0.1779, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 26 tensor(0.1739, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 27 tensor(0.1759, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 28 tensor(0.1620, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 29 tensor(0.1700, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 30, loss is 0.17000198364257812\n",
      "epoch= 30 tensor(0.1668, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 31 tensor(0.1601, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 32 tensor(0.1603, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 33 tensor(0.1608, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 34 tensor(0.1609, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 35, loss is 0.16092011332511902\n",
      "epoch= 35 tensor(0.1704, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 36 tensor(0.1685, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 37 tensor(0.1605, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 38 tensor(0.1623, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 39 tensor(0.1584, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 40, loss is 0.1584099978208542\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 超参数设置\n",
    "    batch_size = 128\n",
    "    lr = 1e-2\n",
    "    weight_decay = 1e-5\n",
    "    epoches = 40\n",
    "    model = autoencoder()\n",
    "    # x = Variable(torch.randn(1, 28*28))\n",
    "    # encode, decode = model(x)\n",
    "    # print(encode.shape)\n",
    "    train_data = get_data()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizier = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    for epoch in range(epoches):\n",
    "        if epoch in [epoches * 0.25, epoches * 0.5]:\n",
    "            for param_group in optimizier.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        for img, _ in train_data:\n",
    "            img = img.view(img.size(0), -1)\n",
    "            img = Variable(img.cuda())\n",
    "            # forward\n",
    "            _, output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            # backward\n",
    "            optimizier.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizier.step()\n",
    "        print(\"epoch=\", epoch, loss.data.float())\n",
    "        for param_group in optimizier.param_groups:\n",
    "            print(param_group['lr'])\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(\"epoch: {}, loss is {}\".format((epoch+1), loss.data))\n",
    "            pic = to_img(output.cpu().data)\n",
    "            if not os.path.exists('./simple_autoencoder'):\n",
    "                os.mkdir('./simple_autoencoder')\n",
    "            save_image(pic, './simple_autoencoder/image_{}.png'.format(epoch + 1))\n",
    "    # torch.save(model, './autoencoder.pth')\n",
    "    # model = torch.load('./autoencoder.pth')\n",
    "    code = Variable(torch.FloatTensor([[1.19, -3.36, 2.06]]).cuda())\n",
    "    decode = model.decoder(code)\n",
    "    decode_img = to_img(decode).squeeze()\n",
    "    decode_img = decode_img.data.cpu().numpy() * 255\n",
    "    plt.imshow(decode_img.astype('uint8'))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 基于卷积网络的自编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 tensor(0.1746, device='cuda:0')\n",
      "0.01\n",
      "epoch= 1 tensor(0.1520, device='cuda:0')\n",
      "0.01\n",
      "epoch= 2 tensor(0.1463, device='cuda:0')\n",
      "0.01\n",
      "epoch= 3 tensor(0.1458, device='cuda:0')\n",
      "0.01\n",
      "epoch= 4 tensor(0.1371, device='cuda:0')\n",
      "0.01\n",
      "epoch: 5, loss is 0.13708260655403137\n",
      "epoch= 5 tensor(0.1431, device='cuda:0')\n",
      "0.01\n",
      "epoch= 6 tensor(0.1422, device='cuda:0')\n",
      "0.01\n",
      "epoch= 7 tensor(0.1365, device='cuda:0')\n",
      "0.01\n",
      "epoch= 8 tensor(0.1376, device='cuda:0')\n",
      "0.01\n",
      "epoch= 9 tensor(0.1335, device='cuda:0')\n",
      "0.01\n",
      "epoch: 10, loss is 0.13348431885242462\n",
      "epoch= 10 tensor(0.1237, device='cuda:0')\n",
      "0.01\n",
      "epoch= 11 tensor(0.1319, device='cuda:0')\n",
      "0.01\n",
      "epoch= 12 tensor(0.1225, device='cuda:0')\n",
      "0.01\n",
      "epoch= 13 tensor(0.1322, device='cuda:0')\n",
      "0.01\n",
      "epoch= 14 tensor(0.1167, device='cuda:0')\n",
      "0.01\n",
      "epoch: 15, loss is 0.11674923449754715\n",
      "epoch= 15 tensor(0.1224, device='cuda:0')\n",
      "0.01\n",
      "epoch= 16 tensor(0.1266, device='cuda:0')\n",
      "0.01\n",
      "epoch= 17 tensor(0.1243, device='cuda:0')\n",
      "0.01\n",
      "epoch= 18 tensor(0.1249, device='cuda:0')\n",
      "0.01\n",
      "epoch= 19 tensor(0.1288, device='cuda:0')\n",
      "0.01\n",
      "epoch: 20, loss is 0.12881611287593842\n",
      "epoch= 20 tensor(0.1295, device='cuda:0')\n",
      "0.01\n",
      "epoch= 21 tensor(0.1217, device='cuda:0')\n",
      "0.01\n",
      "epoch= 22 tensor(0.1322, device='cuda:0')\n",
      "0.01\n",
      "epoch= 23 tensor(0.1227, device='cuda:0')\n",
      "0.01\n",
      "epoch= 24 tensor(0.1228, device='cuda:0')\n",
      "0.01\n",
      "epoch: 25, loss is 0.12281524389982224\n",
      "epoch= 25 tensor(0.1281, device='cuda:0')\n",
      "0.001\n",
      "epoch= 26 tensor(0.1113, device='cuda:0')\n",
      "0.001\n",
      "epoch= 27 tensor(0.1195, device='cuda:0')\n",
      "0.001\n",
      "epoch= 28 tensor(0.1186, device='cuda:0')\n",
      "0.001\n",
      "epoch= 29 tensor(0.1219, device='cuda:0')\n",
      "0.001\n",
      "epoch: 30, loss is 0.12190507352352142\n",
      "epoch= 30 tensor(0.1249, device='cuda:0')\n",
      "0.001\n",
      "epoch= 31 tensor(0.1174, device='cuda:0')\n",
      "0.001\n",
      "epoch= 32 tensor(0.1198, device='cuda:0')\n",
      "0.001\n",
      "epoch= 33 tensor(0.1177, device='cuda:0')\n",
      "0.001\n",
      "epoch= 34 tensor(0.1142, device='cuda:0')\n",
      "0.001\n",
      "epoch: 35, loss is 0.1142406240105629\n",
      "epoch= 35 tensor(0.1167, device='cuda:0')\n",
      "0.001\n",
      "epoch= 36 tensor(0.1186, device='cuda:0')\n",
      "0.001\n",
      "epoch= 37 tensor(0.1138, device='cuda:0')\n",
      "0.001\n",
      "epoch= 38 tensor(0.1182, device='cuda:0')\n",
      "0.001\n",
      "epoch= 39 tensor(0.1200, device='cuda:0')\n",
      "0.001\n",
      "epoch: 40, loss is 0.11995922774076462\n",
      "epoch= 40 tensor(0.1231, device='cuda:0')\n",
      "0.001\n",
      "epoch= 41 tensor(0.1149, device='cuda:0')\n",
      "0.001\n",
      "epoch= 42 tensor(0.1203, device='cuda:0')\n",
      "0.001\n",
      "epoch= 43 tensor(0.1153, device='cuda:0')\n",
      "0.001\n",
      "epoch= 44 tensor(0.1090, device='cuda:0')\n",
      "0.001\n",
      "epoch: 45, loss is 0.10902364552021027\n",
      "epoch= 45 tensor(0.1295, device='cuda:0')\n",
      "0.001\n",
      "epoch= 46 tensor(0.1084, device='cuda:0')\n",
      "0.001\n",
      "epoch= 47 tensor(0.1174, device='cuda:0')\n",
      "0.001\n",
      "epoch= 48 tensor(0.1261, device='cuda:0')\n",
      "0.001\n",
      "epoch= 49 tensor(0.1145, device='cuda:0')\n",
      "0.001\n",
      "epoch: 50, loss is 0.11445703357458115\n",
      "epoch= 50 tensor(0.1173, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 51 tensor(0.1158, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 52 tensor(0.1171, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 53 tensor(0.1128, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 54 tensor(0.1174, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 55, loss is 0.1173698827624321\n",
      "epoch= 55 tensor(0.1157, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 56 tensor(0.1186, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 57 tensor(0.1257, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 58 tensor(0.1101, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 59 tensor(0.1148, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 60, loss is 0.1147901639342308\n",
      "epoch= 60 tensor(0.1158, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 61 tensor(0.1179, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 62 tensor(0.1182, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 63 tensor(0.1143, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 64 tensor(0.1307, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 65, loss is 0.13065728545188904\n",
      "epoch= 65 tensor(0.1161, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 66 tensor(0.1206, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 67 tensor(0.1184, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 68 tensor(0.1129, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 69 tensor(0.1193, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 70, loss is 0.11926788091659546\n",
      "epoch= 70 tensor(0.1121, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 71 tensor(0.1164, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 72 tensor(0.1212, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 73 tensor(0.1114, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 74 tensor(0.1158, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 75, loss is 0.11576029658317566\n",
      "epoch= 75 tensor(0.1070, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 76 tensor(0.1121, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 77 tensor(0.1187, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 78 tensor(0.1120, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 79 tensor(0.1094, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 80, loss is 0.10944736748933792\n",
      "epoch= 80 tensor(0.1152, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 81 tensor(0.1175, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 82 tensor(0.1141, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 83 tensor(0.1158, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 84 tensor(0.1119, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 85, loss is 0.11188703775405884\n",
      "epoch= 85 tensor(0.1067, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 86 tensor(0.1134, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 87 tensor(0.1136, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 88 tensor(0.1086, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 89 tensor(0.1156, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 90, loss is 0.11560797691345215\n",
      "epoch= 90 tensor(0.1042, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 91 tensor(0.1143, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 92 tensor(0.1096, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 93 tensor(0.0998, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 94 tensor(0.1101, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 95, loss is 0.11010667681694031\n",
      "epoch= 95 tensor(0.1060, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 96 tensor(0.1112, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 97 tensor(0.1081, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 98 tensor(0.1026, device='cuda:0')\n",
      "0.0001\n",
      "epoch= 99 tensor(0.1079, device='cuda:0')\n",
      "0.0001\n",
      "epoch: 100, loss is 0.10794036090373993\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# model = torch.load('./autoencoder.pth')\u001b[39;00m\n\u001b[0;32m     88\u001b[0m code \u001b[38;5;241m=\u001b[39m Variable(torch\u001b[38;5;241m.\u001b[39mFloatTensor([[\u001b[38;5;241m1.19\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3.36\u001b[39m, \u001b[38;5;241m2.06\u001b[39m]])\u001b[38;5;241m.\u001b[39mcuda())\n\u001b[1;32m---> 89\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m decode_img \u001b[38;5;241m=\u001b[39m to_img(decode)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m     91\u001b[0m decode_img \u001b[38;5;241m=\u001b[39m decode_img\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SBH\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:952\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[1;34m(self, input, output_size)\u001b[0m\n\u001b[0;32m    947\u001b[0m num_spatial_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m    948\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    950\u001b[0m     num_spatial_dims, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m--> 952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [1, 3]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 加载数据集\n",
    "def get_data():\n",
    "    # 将像素点转换到[-1, 1]之间，使得输入变成一个比较对称的分布，训练容易收敛\n",
    "    data_tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "    train_dataset = datasets.MNIST(root='./data', train=True, transform=data_tf, download=True)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    return train_loader\n",
    "\n",
    "def to_img(x):\n",
    "    x = (x + 1.) * 0.5\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # (b, 16, 10, 10)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # (b, 16, 5, 5)\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # (b, 8, 3, 3)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # (b, 8, 2, 2)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # (b, 16, 5, 5)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # (b, 8, 15, 15)\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # (b, 1, 28, 28)\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encode = self.encoder(x)\n",
    "        decode = self.decoder(encode)\n",
    "        return encode, decode\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 超参数设置\n",
    "    batch_size = 128\n",
    "    lr = 1e-2\n",
    "    weight_decay = 1e-5\n",
    "    epoches = 100\n",
    "    model = autoencoder()\n",
    "    # x = Variable(torch.randn(1, 28*28))\n",
    "    # encode, decode = model(x)\n",
    "    # print(encode.shape)\n",
    "    train_data = get_data()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizier = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "    for epoch in range(epoches):\n",
    "        if epoch in [epoches * 0.25, epoches * 0.5]:\n",
    "            for param_group in optimizier.param_groups:\n",
    "                param_group['lr'] *= 0.1\n",
    "        for img, _ in train_data:\n",
    "            # img = img.view(img.size(0), -1)\n",
    "            img = Variable(img.cuda())\n",
    "            # forward\n",
    "            _, output = model(img)\n",
    "            loss = criterion(output, img)\n",
    "            # backward\n",
    "            optimizier.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizier.step()\n",
    "        print(\"epoch=\", epoch, loss.data.float())\n",
    "        for param_group in optimizier.param_groups:\n",
    "            print(param_group['lr'])\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            print(\"epoch: {}, loss is {}\".format((epoch+1), loss.data))\n",
    "            pic = to_img(output.cpu().data)\n",
    "            if not os.path.exists('./simple_autoencoder'):\n",
    "                os.mkdir('./simple_autoencoder')\n",
    "            save_image(pic, './simple_autoencoder/image_{}.png'.format(epoch + 1))\n",
    "    torch.save(model, './autoencoder.pth')\n",
    "    # model = torch.load('./autoencoder.pth')\n",
    "    code = Variable(torch.FloatTensor([[1.19, -3.36, 2.06]]).cuda())\n",
    "    decode = model.decoder(code)\n",
    "    decode_img = to_img(decode).squeeze()\n",
    "    decode_img = decode_img.data.cpu().data * 255\n",
    "    plt.imshow(decode_img.numpy().astype('uint8'), cmap='gray')\n",
    "    save_image(decode_img, './simple_autoencoder/image_code.png')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Variational AutoEncoder 变分自动编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变分自动编码器（Variational AutoEncoder）是自动编码器的升级版本，它的结构和自动编码器是相似的，也是由编码器和解码器构成的。\n",
    "\n",
    "在自动编码器中，需要一个输入图片，然后将输入图片编码之后得到一个隐含向量，这比随机取一个随机向量好，因为这包含着原图片的信息，然后将隐含信息解码得到与原图片对应的照片。但是这样并不能生成任意图片，因为没办法构造隐含向量，需要通过一张图片输入编码才知道得到的隐含向量是什么，这时就可以通过变分自动编码器解决这个问题。\n",
    "\n",
    "其实原理很简单，只需要在编码过程中给它增加一些限制，迫使它生成的隐含向量能够粗略地遵循一个标准正态分布，这就是它与一般的自动编码器最大的不同。这样生成一张新图片就很简单了，只需要给它一个标准正态分布的随机隐含向量，通过解码器就能够生成想要的图片，而不需要先给它一张原始图片编码。\n",
    "\n",
    "在实际情况中，需要在模型的准确率和隐含向量服从标准正态分布之间做一个权衡，所谓模型的准确率就是指解码器生成的图片与原始图片的相似程度。可以让神经网络自己做这个决定，只需要将两者都做一个loss，然后求和作为总的loss，这样网络就能够自己选择如何做才能使这个总的loss下降。另外要衡量两种分布的相似程度，需要引入一个新的概念，KL divergence，这是用来衡量两种分布相似程度的统计量，它越小，表示两种概率分布越接近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于离散的概率分布，定义如下：\n",
    "$$D_{KL}(P \\parallel Q) = \\sum_i P(i) \\log\\frac{P(i)}{Q(i)}\\$$\n",
    "对于连续的概率分布，定义如下：\n",
    "$$D_{KL}(P \\parallel Q) = \\int_{-\\infty}^{\\infty} p(x) \\log\\frac{p(x)}{q(x)} \\, dx$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里就是用KL divergence 表示隐含向量与标准正态分布之间差异的loss，另外一个loss仍然使用生成图片与原图片的均方误差来表示。\n",
    "\n",
    "这里的变分编码器使用了一个技巧——\"重新参数化\"来解决KL divergence的计算问题。这时不再是每次生成一个隐含向量，而是生成两个向量：一个表示均值，一个表示标准差，然后通过这两个统计量合成隐含向量，用一个标准正太分布先乘标准差再加上均值就行了，这里默认编码之后的隐含向量是服从一个正态分布的。这个时候要让均值尽可能接近0，标准差尽可能接近1。\n",
    "\n",
    "代码如下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import save_image\n",
    "# 加载数据集\n",
    "def get_data():\n",
    "    data_tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "    train_data = datasets.MNIST(root='./data', train=True, transform=data_tf, download=True)\n",
    "    train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "    return train_loader\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 400)\n",
    "        self.fc21 = nn.Linear(400, 20)    # 均值\n",
    "        self.fc22 = nn.Linear(400, 20)    # 方差\n",
    "        self.fc3 = nn.Linear(20, 400)\n",
    "        self.fc4 = nn.Linear(400, 784)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        mu = self.fc21(h1)\n",
    "        logvar = self.fc22(h1)\n",
    "        return mu, logvar\n",
    "\n",
    "    def decoder(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        x = F.tanh(self.fc4(h3))\n",
    "        return x\n",
    "    # 重新参数化\n",
    "    def reparametrize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()   # 计算标准差\n",
    "        if torch.cuda.is_available():\n",
    "            eps = torch.cuda.FloatTensor(std.size()).normal_()    # 从标准的正态分布中随机采样一个eps\n",
    "        else:\n",
    "            eps = torch.FloatTensor(std.size()).normal_()\n",
    "        eps = Variable(eps)\n",
    "        return eps.mul(std).add_(mu)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encoder(x)\n",
    "        z = self.reparametrize(mu, logvar)\n",
    "        return self.decoder(z), mu, logvar\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    MSE = reconstruction_function(recon_x, x)\n",
    "    # loss = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
    "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
    "    # KL divergence\n",
    "    return MSE + KLD\n",
    "\n",
    "def to_img(x):\n",
    "    x = (x + 1.) * 0.5\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 超参数设置\n",
    "    batch_size = 128\n",
    "    lr = 1e-3\n",
    "    epoches = 100\n",
    "\n",
    "    model = VAE()\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    train_data = get_data()\n",
    "\n",
    "    reconstruction_function = nn.MSELoss(reduction='sum')\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epoches):\n",
    "        for img, _ in train_data:\n",
    "            img = img.view(img.size(0), -1)\n",
    "            img = Variable(img)\n",
    "            if torch.cuda.is_available():\n",
    "                img = img.cuda()\n",
    "            # forward\n",
    "            output, mu, logvar = model(img)\n",
    "            loss = loss_function(output, img, mu, logvar)/img.size(0)\n",
    "            # backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\"epoch=\", epoch, loss.data.float())\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(\"epoch = {}, loss is {}\".format(epoch+1, loss.data))\n",
    "            pic = to_img(output.cpu().data)\n",
    "            if not os.path.exists('./vae_img1'):\n",
    "                os.mkdir('./vae_img1')\n",
    "            save_image(pic, './vae_img1/image_{}.png'.format(epoch + 1))\n",
    "    torch.save(model, './vae.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行100个eopch之后，结果如下，可以看出来结果比上面的自动编码器清晰一点，本质上VAE就是在encoder的结果添加了高斯噪声，通过训练要使得decoder对噪声有一定的鲁棒性，这样的话我们生成一张图片就没有必须用一张图片先做编码了，可以想象，我们只需要利用训练好的encoder对一张图片编码得到其分布后，符合这个分布的隐含向量理论上都可以通过decoder得到类似这张图片的图片。\n",
    "\n",
    "KL越小，噪声越大（可以这麽理解，我们强行让z的分布符合正态分布，其和N(0,1)越接近，KL越小，相当于我们添加的噪声越大），所以直觉上来想loss合并后的训练过程：\n",
    "\n",
    "- 当decoder 还没有训练好时（重构误差远大于KLloss），就会适当降低噪声（KLloss增加），使得拟合起来容易一些（重构误差开始下降）；\n",
    "\n",
    "- 反之，如果decoder训练得还不错时（重构误差小于KLloss），这时候噪声就会增加（KLloss减少），使得拟合更加困难了（重构误差又开始增加），这时候decoder就要想办法提高它的生成能力了。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
