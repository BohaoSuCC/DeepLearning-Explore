{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (Long Short Term Memory) \n",
    "\n",
    "LSTM is a type of RNN (Recurrent Neural Network) that is capable of learning long-term dependencies. It is widely used in time series forecasting, natural language processing, and other applications.\n",
    "\n",
    "LSTM是一种RNN（循环神经网络），它能够学习长期依赖关系。它广泛应用于时间序列预测、自然语言处理等应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. LSTM Structure\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327195559364.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM网络在训练时会使用上一时刻的信息，加上本次时刻的输入信息来共同训练。\n",
    "\n",
    "举个简单的例子：在第一天我生病了（初始状态$H_0$），然后吃药（利用输入信息$X_1$训练网络），第二天好转但是没有完全好$H_1$，再吃药$X_2$,病情得到好转$H_2$,如此循环往复直到病情好转。因此，输入$X_t$是吃药，时间轴T是吃多天的药，隐含层状态是病情状况。因此我还是我，只是不同状态的我。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的图表示包含2个隐含层的LSTM网络，在$T=1$时刻看，它是一个普通的BP网络，在$T=2$时刻看也是一个普通的BP网络，只是沿时间轴展开后，$T=1$训练的隐含层信息H,C会被传递到下一个时刻$T=2$，如下图所示。上图中向右的五个常常的箭头，所的也是隐含层状态在时间轴上的传递。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, $H$ represents the hidden layer state, and $C$ is the forget gate.\n",
    "\n",
    "图中$H$表示隐藏层状态，$C$是遗忘门\n",
    "\n",
    "实际上遗忘门比作记忆门Remember gate更合适，因为它不是遗忘，而是记忆。因为这一层如果是1，就是记忆，如果是0，就是遗忘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LSTM Input Structure\n",
    "\n",
    "为了更好理解LSTM结构，还必须理解LSTM的数据输入情况。仿照3通道图像的样子，在加上时间轴后的多样本的多特征的不同时刻的数据立方体如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/20210327201559465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "右边的图是我们常见模型的输入，比如XGBOOST，lightGBM，决策树等模型，输入的数据格式都是这种（$NF$）的矩阵，而左边是加上时间轴后的数据立方体，也就是时间轴上的切片，它的维度是（$NT*F$）,第一维度是样本数，第二维度是时间，第三维度是特征数，如下图所示：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327201821362.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样的数据立方体很多，比如天气预报数据，把样本理解成城市，时间轴是日期，特征是天气相关的降雨风速PM2.5等，这个数据立方体就很好理解了。在NLP里面，一句话会被embedding成一个矩阵，词与词的顺序是时间轴T，索引多个句子的embedding三维矩阵如下图所示：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327201900507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM Code by Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LSTM in Pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch中定义的LSTM模型的参数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch.nn.LSTM(*args, **kwargs)\n",
    "\"\"\"\n",
    "参数有：\n",
    "    input_size:,       #x的特征维度\n",
    "    hidden_size:,      #隐藏层的特征维度\n",
    "    num_layers:,       #lstm隐层的层数,默认为1\n",
    "    bias:              #False则bihbih=0和bhhbhh=0. 默认为True\n",
    "    batch_first:       #True则输入输出的数据格式为 (batch, seq, feature)\n",
    "    dropout:           #除最后一层,每一层的输出都进行dropout,默认为: 0\n",
    "    bidirectional:     #True则为双向lstm默认为False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_size：x的特征维度，就是数据立方体中的F，在NLP中就是一个词被embedding后的向量长度，如下图所示：\n",
    "![](https://img-blog.csdnimg.cn/20210327202457470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "- hidden_size：隐藏层的特征维度（隐藏层神经元个数），如下图所示，我们有两个隐含层，每个隐藏层的特征维度都是5。注意，非双向LSTM的输出维度等于隐藏层的特征维度。\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327202534441.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)\n",
    "\n",
    "- num_layers：lstm隐层的层数，上面的图我们定义了2个隐藏层。\n",
    "\n",
    "- batch_first：用于定义输入输出维度，后面再讲。\n",
    "\n",
    "- bidirectional：是否是双向循环神经网络，如下图是一个双向循环神经网络，因此在使用双向LSTM的时候我需要特别注意，正向传播的时候有（$H_t$, $C_t$）,反向传播也有（$H_t^\\prime$, $C_t^\\prime$）,前面我们说了非双向LSTM的输出维度等于隐藏层的特征维度，而双向LSTM的输出维度是隐含层特征数2，而且$H,C$的维度是时间轴长度2。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 输入进LSTM的数据格式 Data Format for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input(seq_len, batch, input_size)\n",
    "\"\"\"\n",
    "参数有：\n",
    "    seq_len:            #序列长度，在NLP中就是句子长度，一般都会用pad_sequence补齐长度\n",
    "    batch:              #每次喂给网络的数据条数，在NLP中就是一次喂给网络多少个句子\n",
    "    input_size:         #特征维度，和前面定义网络结构的input_size一致。\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面也说到，如果LSTM的参数 batch_first=True，则要求输入的格式是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input(batch, seq_len, input_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "刚好调换前面两个参数的位置。其实这是比较好理解的数据形式，下面以NLP中的embedding向量说明如何构造LSTM的输入。\n",
    "\n",
    "之前我们的embedding矩阵如下图：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327203625378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果把batch放在第一位，则三维矩阵的形式如下：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327203644310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其转换过程如下图所示：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327204302953.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM的另外两个输入是 $h_0$ 和 $c_0$，可以理解成网络的初始化参数，用随机数生成即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "h0(num_layers * num_directions, batch, hidden_size)\n",
    "c0(num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "\n",
    "参数：\n",
    "    num_layers：隐藏层数\n",
    "    num_directions：如果是单向循环网络，则num_directions=1，双向则num_directions=2\n",
    "    batch：输入数据的batch\n",
    "    hidden_size：隐藏层神经元个数\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，如果我们定义的input格式是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input(batch, seq_len, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则$H$和$C$的格式也是要变的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#h0(batch, num_layers * num_directions,  hidden_size)\n",
    "#c0(batch, num_layers * num_directions,  hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 LSTM output format 输出的格式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM的输出是一个tuple，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output,(ht, ct) = net(input)\n",
    "    output:            #最后一个状态的隐藏层的神经元输出\n",
    "    ht:                #最后一个状态的隐含层的状态值\n",
    "    ct:                #最后一个状态的隐含层的遗忘门值\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output的默认维度是："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output(seq_len, batch, hidden_size * num_directions)\n",
    "ht(num_layers * num_directions, batch, hidden_size)\n",
    "ct(num_layers * num_directions, batch, hidden_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和input的情况类似，如果我们前面定义的input格式是：\n",
    "\n",
    "则$h_t$和$c_t$的格式也是要变的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#若input的维度为\n",
    "input(batch, seq_len, input_size)\n",
    "# 则对应的h0和c0的维度为\n",
    "ht(batc，num_layers * num_directions, h, hidden_size)\n",
    "ct(batc，num_layers * num_directions, h, hidden_size)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Summary\n",
    "\n",
    "我们回过头来看看ht和ct在哪里，请看下图：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327204947654.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output在哪里？请看下图：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327205100524.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 LSTM combined with other networks LSTM与其他网络的组合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output的维度等于隐藏层神经元的个数，即hidden_size，在一些时间序列的预测中，会在output后，接上一个全连接层，全连接层的输入维度等于LSTM的hidden_size，之后的网络处理就和BP网络相同了，如下图：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/20210327205155470.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MTc0NDE5Mg==,size_16,color_FFFFFF,t_70#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 LSTM pytorch实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#用pytorch实现以上结构\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RegLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegLSTM, self).__init__()\n",
    "        # 定义LSTM\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, hidden_num_layers)\n",
    "        # 定义回归层网络，输入的特征维度等于LSTM的输出，输出维度为1\n",
    "        self.reg = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, (ht,ct) = self.rnn(x)\n",
    "        seq_len, batch_size, hidden_size= x.shape\n",
    "        x = x.view(-1, hidden_size)\n",
    "        x = self.reg(x)\n",
    "        x = x.view(seq_len, batch_size, -1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "和RNN类似，LSTM也有两种使用方式，直接调用nn.LSTM和详细分步骤的nn.LSTMCell，下面是两种方式的代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(100, 20, num_layers=4)\n",
      "torch.Size([10, 3, 20]) torch.Size([4, 3, 20]) torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 直接调用nn.LSTM，无需分步控制\n",
    "\n",
    "lstm = nn.LSTM(input_size=100, hidden_size=20, num_layers=4)\n",
    "print(lstm)\n",
    "\n",
    "x=torch.randn(10, 3, 100)\n",
    "out, (h, c) = lstm(x)\n",
    "\n",
    "print(out.shape, h.shape, c.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二种实现方式，使用nn.LSTMCell，代码如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One layer LSTMcell\n",
      "torch.Size([3, 20]) torch.Size([3, 20])\n",
      "Two layer LSTMcell\n",
      "torch.Size([3, 20]) torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# 用nn.LSTMCell实现一个one-layer LSTM\n",
    "\n",
    "print(\"One layer LSTMcell\")\n",
    "\n",
    "cell = nn.LSTMCell(input_size=100, hidden_size=20)\n",
    "h = torch.zeros(3, 20)\n",
    "c = torch.zeros(3, 20)\n",
    "\n",
    "for xt in x:\n",
    "    h, c = cell(xt, [h, c])\n",
    "\n",
    "print(h.shape, c.shape)\n",
    "\n",
    "\n",
    "\n",
    "#用nn.LSTMCell实现一个two-layer LSTM\n",
    "\n",
    "print(\"Two layer LSTMcell\")\n",
    "\n",
    "cell1 = nn.LSTMCell(input_size=100, hidden_size=30)\n",
    "cell2 = nn.LSTMCell(input_size=30, hidden_size=20)\n",
    "h1 = torch.zeros(3, 30)\n",
    "c1 = torch.zeros(3, 30)\n",
    "h2 = torch.zeros(3, 20)\n",
    "c2 = torch.zeros(3, 20)\n",
    "\n",
    "for xt in x:\n",
    "    h1, c1 = cell1(xt, [h1, c1])\n",
    "    h2, c2 = cell2(h1, [h2, c2])\n",
    "\n",
    "print(h2.shape, c2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM CASE 实战案例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Sentiment Classification 情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext.legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data, datasets\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext.legacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from torchtext.legacy import data, datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchtext.data' has no attribute 'Field'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m TEXT \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mField\u001b[49m(tokenize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m LABEL \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mLabelField(dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m      4\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mIMDB\u001b[38;5;241m.\u001b[39msplits(TEXT, LABEL)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"
     ]
    }
   ],
   "source": [
    "TEXT = data.Field(tokenize='spacy')\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "\n",
    "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
