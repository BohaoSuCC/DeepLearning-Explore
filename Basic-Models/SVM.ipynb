{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM (Support Vector Machine) Classifier 支持向量机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, the so-called support vector machine is a classifier. \n",
    "\n",
    "通俗来讲，所谓支持向量机是一种分类器，\n",
    "\n",
    "For two sets of vectors that have been marked, it gives an optimal separation hyperplane to separate these two sets of vectors, so that the vectors closest to this hyperplane in the two sets of vectors (i.e., the so-called support vectors) are as far away from this hyperplane as possible.\n",
    "\n",
    "对于做出标记的两组向量，给出一个最优分割超曲面把这两组向量分割到两边，使得两组向量中离此超平面最近的向量（即所谓支持向量）到此超平面的距离都尽可能远。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initial Goal 初步目标\n",
    "\n",
    "### 1.1 Hyperplane Equation 超平面方程\n",
    "\n",
    "The vector(tensor), could be represented as a point in the space, and the hyperplane could be represented as a equation in the space.\n",
    "\n",
    "Tensor could be understood as a arrow starting from the origin in the Cartesian coordinate system, pointing to a certain coordinate point, so it has only two features, size and direction. \n",
    "\n",
    "所谓向量，可以简单理解成在直角坐标系中由原点出发，指向某一坐标点的一个剪头，因此它只有两个特征，大小和方向.\n",
    "\n",
    "\n",
    "\n",
    "The so-called $n$-dimensional hyperplane is a set of vectors in the $n$-dimensional inner product space, all of which have the same projection length on the same normal vector (such as $w$ in the figure below).\n",
    "\n",
    "所谓$n$维超平面（hyperplane）是指在$n$维内积空间中，所有在同一个法向量（如下图中$w$）上投影长度相同的向量组成的集合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Vector \n",
    "\n",
    "$$\\mathbf{w} = [w_1 \\ w_2 \\ ... \\ w_n]^T$$\n",
    "\n",
    "The norm of $w$ is \n",
    "\n",
    "$$\\|\\mathbf{w}\\| = \\sqrt{w_1^2 + w_2^2 + ... + w_n^2}$$\n",
    "\n",
    "\n",
    "The hyperplane equation is $w^T x + b = 0$, where $x$ is the input vector, $b$ is the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All vectors on the hyperplane has the same projection length on the normal vector as $l$, which is the distance from the origin to the hyperplane. So, by the definition of inner product, the absolute value of the inner product of any vector $\\mathbf{x} = [x_1 \\ x_2 \\ ... \\ x_n]^T$ on the hyperplane and the normal vector $w$ is the product of its projection in the direction of the normal vector and the length of the normal vector.$$\\mathbf{w}^T\\mathbf{x} = |w_1x_1 + w_2x_2 + ... + w_nx_n| = al$$\n",
    "\n",
    "超平面上的所有向量在法向量方向上的投影长度都相同，记为l，即原点到此超平面的距离，所以由内积定义可知，超平面上的任一向量$\\mathbf{x} = [x_1 \\ x_2 \\ ... \\ x_n]^T$与此法向量w的内积绝对值就是其在法向量方向投影与法向量模长的乘积, 即\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because once the normal vector and the hyperplane are fixed, $a$ and $l$ are fixed values, so the above formula can be rewritten as $-b$, and the meaning of $b > 0$ is that the projection direction of the vector $x$ is opposite to the normal vector of the hyperplane, so the hyperplane equation is obtained:\n",
    "\n",
    "$$\\mathbf{w}^T\\mathbf{x} + b = 0$$\n",
    "\n",
    "And b is socalled bias/offset/interecept of the hyperplane.\n",
    "\n",
    "由于一旦法向量和超平面固定，a和l就都是固定值，所以可将上式中$al$改写成$−b$，并赋予$b>0$的含义是向量$x$投影方向与超平面在法向量反方向，于是就得到了超平面方程：\n",
    "\n",
    "b即为超平面的偏置/偏移/截距。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, for a vector $y$ outside the hyperplane, the absolute value of the inner product of the vector and the normal vector is:\n",
    "\n",
    "$$\\mathbf{w}^T\\mathbf{y} = |w_1y_1 + w_2y_2 + ... + w_ny_n| = ad$$\n",
    "\n",
    "对于超平面外一向量$y$,同理可知该向量与超平面法向量的内积绝对值为：\n",
    "\n",
    "And the distance from the vector $y$ to the hyperplane is $h$, so we have the following relationship:\n",
    "\n",
    "$$h = d - l = \\frac{ad - al}{a} = \\frac{|\\mathbf{w}^T\\mathbf{y} + b|}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "而在上图中可直观看出向量$y$到此超平面的距离是$h$,且有如下关系 \n",
    "\n",
    "Therefore, the geometric distance from a vector outside the hyperplane to the hyperplane is equal to the absolute value of the vector being substituted into the hyperplane equation, and then divided by the modulus of the normal vector of the hyperplane.\n",
    "\n",
    "由此可见，超平面外一向量到超平面的几何距离就等于将该向量代入超平面方程取绝对值后，再除以超平面法向量的模长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Strictly Linear Separable Problem 严格线性可分问题 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that there are $m$ vectors $x_1-x_m$ in an $n$-dimensional vector space, and each vector $x_i$ corresponds to a label $y_i$, where the label value of some vectors is 1, and the label value of the other vectors is -1, so as to distinguish two groups of vectors. What we have to do is to construct a hyperplane in the $n$-dimensional vector space, so that the support vectors found on both sides of the hyperplane (i.e., the vectors $x_s$ closest to the hyperplane in geometric distance) are as far away from this hyperplane as possible. The distance from the support vector to the hyperplane is:\n",
    "\n",
    "设$n$维向量空间中有$m$个向量$x_1-x_m$，每个向量$x_i$都对应一个标签$y_i$，其中一部分向量的标签值是1，另一部分向量的标签值是-1，从而区分出两组向量。我们所要做的是在$n$维向量空间中构造一个超平面，使得在超平面两侧找到的支持向量（即距离超平面几何距离最短的向量$x_s$到此超平面的距离尽可能远，由前述超平面方程可知这个距离为：\n",
    "\n",
    "$$d_s = \\frac{|\\mathbf{w}^T \\mathbf{x}_s + b|}{\\|\\mathbf{w}\\|}$$\n",
    "\n",
    "At the same time, another constraint should be satisfied, that is, the vectors with the same label should only be on the same side of this hyperplane, that is, the following relationship should be satisfied: \n",
    "\n",
    "同时要满足同标记的向量只能在此超平面的同一边，即:\n",
    "\n",
    "$$\\begin{cases} \n",
    "\\mathbf{w}^T \\mathbf{x}_i + b > 0 & y_i = 1 \\\\\n",
    "\\mathbf{w}^T \\mathbf{x}_i + b \\leq 0 & y_i = -1 \n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The hyperplane satisfies the above two conditions, and the hyperplane is called the **decision plane**.\n",
    "\n",
    "满足上述条件的超平面称为决策面。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Soft Margin 兼容软间隔 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the real world, there are always some abnormal data, such as some positive vectors running into the negative vector heap, if strict classification will cause some vectors to be very close to the decision plane, or even impossible to separate. As shown in the figure below:\n",
    "\n",
    "在实际问题中，总有可能偶尔出现异常数据，比如个别正标向量跑到了负标向量堆里去了，如果严格分类会造成一部分向量与决策面很近，甚至根本无法分界。如下图所示：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/7d9ed742f5c34df4a27dc5e05b33ef13.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWFlfMDIwOQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to tolerate some abnormal data, improve the robustness of the model, and achieve the classification effect in the figure below:\n",
    "\n",
    "为了容忍个别异常数据，提升模型鲁棒性，实现下图分类效果：\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/7071edef3bf94e32ad2d66a48f3b9a78.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWFlfMDIwOQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And need to adjust the original condition extreme value problem as follows:\n",
    "\n",
    "需要对原条件最值问题做调整如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\max_{\\mathbf{w}} \\left\\{ \\frac{1}{\\|\\mathbf{w}\\|} - C \\sum_{i=1}^{m} \\xi_i \\right\\} \\\\\n",
    "& \\text{s.t.} \\quad y_i (\\mathbf{w}^T \\mathbf{x}_i + b) \\geq 1 - \\xi_i \\\\\n",
    "& \\xi_i \\geq 0 \\\\\n",
    "& i = 1,2,...,m\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two new variables greater than 0 are added,\n",
    "其中新增了两个大于0的变量：\n",
    "\n",
    "slack variable $\\xi_i$, when $\\xi_i = 0$, it means that the $i$th vector must strictly follow the classification requirements; when $0<\\xi_i<1$, it means that the $i$th vector is allowed to be closer to the decision plane than the support vector; when $\\xi_i≥1$, it means that the $i$th vector is allowed to be on the decision plane or even on the other side of the decision plane.\n",
    "\n",
    "松弛变量$\\xi_i$。当$\\xi_i = 0$时，说明第$i$个向量要严格遵守分类要求；当$0<\\xi_i<1$时，说明允许第$i$个向量离决策面的距离比支持向量还近；当$\\xi_i≥1$时，表示允许第$i$个向量可以处在决策面上甚至在决策面另一边。\n",
    "\n",
    "Penalty coefficient $C$. If the value of the slack variable is not constrained, it is equivalent to the optimization problem no longer having constraints, so this is obviously not possible, so it is necessary to make each slack variable pay a price, that is, to have a negative impact on the optimization problem, and the degree of this impact is the penalty coefficient, the larger the coefficient, the less tolerant the exception vector, the smaller the coefficient, the more tolerant.\n",
    "\n",
    "惩罚系数$C$。如果松弛变量的取值不被约束，则相当于此优化问题不再有约束条件了，这样显然不行，因此需要使每个松弛变量都付出代价，即对优化问题产生负面影响，这个影响度就是惩罚系数，该系数越大则越不能容忍例外向量，越小则约能容忍。故而该系数是超参数，即需要人为设定，而非模型训练出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compatible with Nonlinear Segmentation 兼容非线性分割 \n",
    "\n",
    "When the classification problem is not linearly separable, the above method is not applicable. For example, when the classification problem is as shown in the figure below, no matter how tolerant the exception vector is, it is impossible to use a straight line to separate the two classes of vectors on the plane.\n",
    "\n",
    "当出现类似下图的分类问题时，无论如何容忍异常向量，也无法做到在平面上用一条直线分割两类向量。\n",
    "\n",
    "![](https://img-blog.csdnimg.cn/9aff9741be68426d8caf7ae1a39c8229.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWFlfMDIwOQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the vector distribution in the figure above, in fact, a circular line is needed to effectively separate the two classes of vectors. Taking the quadratic curve on the plane as an example, its equation form is:\n",
    "\n",
    "对于上图中的向量分布，实际上需要一条环线才能有效分割两类向量。以平面上二次曲线为例，其方程形式为：\n",
    "\n",
    "$$w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1^2 + w_4 x_2^2 + w_5 x_1 x_2 = 0$$\n",
    "\n",
    "Just the same as the linear equation after mapping the original two-dimensional vector to the five-dimensional space.\n",
    "\n",
    "就相当于将原二维向量映射到五维空间后的线性方程，即\n",
    "\n",
    "$$\\mathbf{w}^T \\phi(\\mathbf{x}) + b = 0$$\n",
    "\n",
    "Which is:\n",
    "\n",
    "$$\\phi(\\mathbf{x}) = \\begin{bmatrix}x_1 \\\\x_2 \\\\x_1^2 \\\\x_2^2 \\\\x_1 x_2\\end{bmatrix}, \n",
    "\\quad\\mathbf{w} = \\begin{bmatrix}w_1 \\\\w_2 \\\\w_3 \\\\w_4 \\\\w_5\\end{bmatrix}, \\quad b = w_0\n",
    "$$\n",
    "\n",
    "As shown in the figure below, the two sets of vectors that cannot be linearly separated in low-dimensional space can be mapped to high-dimensional space to achieve linear separation. In order to be compatible with nonlinear segmentation problems, it is necessary to adjust the original condition extreme value problem again, and the final goal is as follows:\n",
    "\n",
    "由此可见，在低维空间无法线性分割的两组向量可以映射到高维空间实现线性分割。为兼容非线性分割问题，需要再次对上述条件最值问题做调整，得到最终目标如下：\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\max_{\\mathbf{w}} \\left\\{ \\frac{1}{\\|\\mathbf{w}\\|} - C \\sum_{i=1}^{m} \\xi_i \\right\\} \\\\\n",
    "& \\text{s.t.} \\quad y_i \\left(\\mathbf{w}^T \\phi(\\mathbf{x}_i) + b\\right) \\geq 1 - \\xi_i, \\\\\n",
    "& \\xi_i \\geq 0, \\\\\n",
    "& i = 1,2,...,m\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Until now, we have finally described the SVM model that is both compatible with soft margin and nonlinear segmentation as a conditional extreme value problem. The next step is how to solve this conditional extreme value problem.\n",
    "\n",
    "至此，终于将既兼容软间隔，又兼容非线性分割的SVM模型描述成了条件最值问题。后面就是如何求解此条件最值问题了。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Lagrange Multiplier Method 拉格朗日乘数法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate the gradient of the objective function, the original condition extreme value problem is transformed into the following Lagrange function:\n",
    "\n",
    "为便于构造拉格朗日函数后计算梯度，可将原条件最值问题等价为：\n",
    "\n",
    "$$\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\min_{\\mathbf{w}} \\left\\{ \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{m} \\xi_i \\right\\} \\\\\n",
    "& \\text{s.t.} \\quad 1 - \\xi_i - y_i \\left(\\mathbf{w}^T \\phi(\\mathbf{x}_i) + b\\right) \\leq 0, \\\\\n",
    "& \\xi_i \\geq 0, \\\\\n",
    "& i = 1,2,...,m\n",
    "\\end{aligned}\n",
    "\\end{equation}$$\n",
    "\n",
    "The Lagrange function has the effect of encapsulating all the constraint conditions into a multivariate function, and then obtaining the desired condition extreme value by finding the extreme value of this multivariate function. We only need to know that the so-called Lagrange function is a polynomial composed of all the condition functions multiplied by a coefficient less than zero, and then added to the objective function to be minimized. For the above condition extreme value problem, the Lagrange function can be constructed as follows:\n",
    "\n",
    "拉格朗日函数的作用是可以把所有约束条件囊括进一个多元函数，通过求该多元函数的最值来得到想要的条件最值。我们只需知道所谓拉格朗日函数就是将所有以小于零作为条件的各条件函数分别乘上一个系数，再加上要求最小值的目标函数后组成的多项式。针对上述条件最值问题，可构造拉格朗日函数如下：\n",
    "\n",
    "$$L(w, \\xi, b, a, \\mu) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{m} \\xi_i + \\sum_{i=1}^{m} a_i \\left\\{1 - \\xi_i - y_i \\left[\\mathbf{w}^T \\phi(\\mathbf{x}_i) + b\\right]\\right\\} - \\sum_{i=1}^{m} \\mu_i \\xi_i$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the optimal goal is to solve the minimum value, and the constraint condition is a series of inequalities, if the gradient of the $L$ function is taken to be zero, it is equivalent to finding the minimum value on the constraint boundary, but what we want is that the minimum value can be in the constraint area, not necessarily only on the boundary. That is to say, for these two cases, we want the effect to be:\n",
    "\n",
    "- If it happens to fall within a few constraint areas, then these few constraint conditions can be ignored, that is, the condition coefficients in front of them can be 0;\n",
    "- If it falls outside a few constraint areas, then these few constraint conditions should take effect and limit the value point to only be on the boundary of the condition.\n",
    "\n",
    "由于优化的目标是求条件最小值，而约束条件又是一系列不等式，如果直接对上述$L$函数取梯度为零点，则相当于在约束边界上找最小值，但我们想要的效果是最小值只要在约束区域内就行，不一定只能在边界上。也就是说通过$L$函数中$(w,\\xi,b)$变量（即除去约束条件前的系数变量）求得的最小值位置可能落在约束区域内也可能落在外面，对于这两种情况，我们想要的效果是：\n",
    "\n",
    "- 如果恰好就落在哪几个约束区域内，则这几个约束条件就可以无视了，即其前面的条件系数就可以为0；\n",
    "- 如果落在哪几个约束区域外，则这几个约束条件就要起作用，把取值点限定在只能在该条件边界上。\n",
    "\n",
    "Therefore, the ultimate goal could be described as follows:\n",
    "\n",
    "因此，最终目标可描述如下：\n",
    "\n",
    "$$\\min_{w,\\xi,b} \\max_{a \\geq 0,\\mu \\geq 0} L(w, \\xi, b, a, \\mu)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Lagrangian duality, taking the minimum of the maximum results and taking the maximum of the minimum results are equivalent. Therefore, the above problem is equivalent to:\n",
    "\n",
    "根据拉格朗日对偶性，取最大结果中的最小值和取最小值中的最大结果是等价的，因此上述问题等价于：\n",
    "$$\\max_{a \\geq 0,\\mu \\geq 0} \\min_{w,\\xi,b} L(w, \\xi, b, a, \\mu)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimazing $L$ Function 对$L$函数优化\n",
    "\n",
    "First step is for minimum value, take the partial derivative of the $L$ function with respect to $w$, $\\xi$, and $b$, and set it to 0, we can get:\n",
    "\n",
    "第一步先取最小值，通过使$L$函数分别对各自变量的偏导为零可得，即\n",
    "\n",
    "$$\\nabla_{w,\\xi,b} L = \n",
    "\\begin{cases}\n",
    "\\frac{\\partial L}{\\partial w} = \\mathbf{w} - \\sum_{i=1}^{m} a_i y_i \\phi(\\mathbf{x}_i) = 0 \\\\\n",
    "\\frac{\\partial L}{\\partial b} = - \\sum_{i=1}^{m} a_i y_i = 0 \\\\\n",
    "\\frac{\\partial L}{\\partial \\xi_i} = C - a_i - \\mu_i = 0\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By substituting this into the optimization objective, and then we can get:\n",
    "\n",
    "将其带入优化目标，可以得到：\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\min_{w,\\xi,b} L(w, \\xi, b, a, \\mu) &= \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^{m} \\xi_i + \\sum_{i=1}^{m} a_i \\left\\{1 - \\xi_i - y_i \\left[\\mathbf{w}^T \\phi(\\mathbf{x}_i) + b\\right]\\right\\} - \\sum_{i=1}^{m} \\mu_i \\xi_i \\\\\n",
    "&= \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} + C \\sum_{i=1}^{m} \\xi_i - \\sum_{i=1}^{m} a_i - \\sum_{i=1}^{m} a_i \\xi_i + \\sum_{i=1}^{m} a_i y_i \\left[\\mathbf{w}^T \\phi(\\mathbf{x}_i) + b\\right] - \\sum_{i=1}^{m} C \\xi_i + \\sum_{i=1}^{m} a_i \\xi_i \\\\\n",
    "&= \\sum_{i=1}^{m} a_i - \\frac{1}{2} \\mathbf{w}^T \\mathbf{w} - \\sum_{i=1}^{m} \\sum_{j=1}^{m} a_i a_j y_i y_j \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second step is to solve $\\max_{a \\geq 0,\\mu \\geq 0} \\min_{w,\\xi,b} L(w, \\xi, b, a, \\mu)$, and the original one could be transformed into the following dual problem:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\min_{a} \\quad & \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} a_i a_j y_i y_j \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) - \\sum_{i=1}^{m} a_i \\\\\n",
    "\\text{s.t.} \\quad & \\sum_{i=1}^{m} a_i y_i = 0 \\\\\n",
    "& a_i, \\mu_i \\geq 0 \\\\\n",
    "& C - a_i - \\mu_i = 0 \\\\\n",
    "& i = 1,2,...,m\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to :\n",
    "\n",
    "由于：\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mu_i &\\geq 0 \\quad \\Rightarrow \\quad a_i \\leq C \\\\\n",
    "\\mu_i &= C - a_i\n",
    "\\end{align*}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could get the final constrained optimization problem:\n",
    "\n",
    "最终得到的约束优化问题为：\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\min_{a} \\quad & \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} a_i a_j y_i y_j \\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) - \\sum_{i=1}^{m} a_i \\\\\n",
    "\\text{s.t.} \\quad & \\sum_{i=1}^{m} a_i y_i = 0 \\\\\n",
    "& 0 \\leq a_i \\leq C \\\\\n",
    "& i = 1,2,...,m\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "上述问题涉及映射到高维空间的向量内积，即$\\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j)$的计算问题，需要引入**核函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Kernel Function 核函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example in Chapter 4, the quadratic curve segmentation of two-dimensional vectors requires mapping to a five-dimensional space. If three-dimensional or even higher-dimensional vectors do high-degree hyper-surface segmentation, the dimension to be raised will be very high, resulting in a huge amount of calculation. \n",
    "\n",
    "在第四章的例子中，对二维向量做二次曲线分割就需要映射到五维空间，如果三维甚至更高维度向量做高次超曲面分割，所要升的维度将很高，从而造成极大的计算量。\n",
    "\n",
    "Therefore, it is necessary to find a method that makes two low-dimensional vectors not need to be mapped to a high-dimensional space, but only need to do simple operations in the original dimensional space, and the result is equal to the inner product after mapping to a high-dimensional space. \n",
    "\n",
    "因此需找到一种方法，使得两个低维向量无需向高维空间做映射，只需在原维度空间做简单运算，所得结果就等于其映射到高维空间后的内积。\n",
    "\n",
    "This method is the kernel function, which takes two low-dimensional vectors and substitutes them into the kernel function, and the result is the inner product of the two vectors after mapping to a high-dimensional space, that is:\n",
    "\n",
    "这种方法就是核函数，将同是低维度的两个向量代入核函数，所得结果就是其映射到高维空间后的内积，即:\n",
    "\n",
    "$$\\phi(\\mathbf{x}_i)^T \\phi(\\mathbf{x}_j) = \\kappa(\\mathbf{x}_i, \\mathbf{x}_j)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some common kernel functions:\n",
    "\n",
    "常用的核函数有以下几种：\n",
    "\n",
    "- Linear Kernel 线性核函数 : \n",
    "\n",
    "  - Representing the origin inner product of the two vectors, that is, the inner product of the two vectors in the original space, which is the simplest kernel function. Usaully used in the case of linearly separable problem.\n",
    "\n",
    "  - 表示两个向量的原始内积，即原空间中两个向量的内积，是最简单的核函数。经常用于线性可分问题。\n",
    "\n",
    "  $$\\kappa(\\mathbf{x}_i, \\mathbf{x}_j) = \\mathbf{x}_i^T \\mathbf{x}_j$$\n",
    "\n",
    "- Gaussian Kernel (Radial Basis Function, RBF)高斯核函数(径向基函数) :\n",
    "\n",
    "  - It is a kernel function that maps the original space to an infinite-dimensional space, and is often used in the case of nonlinear separable problems without emprical experience. And the smaller the ${\\sigma}$, every data point in the high-dimensional space is narrower, the sharper the peak is formed, the more strict the similarity is, and the easier it is to overfit.\n",
    "\n",
    "  - 是将原空间映射到无穷维空间的核函数，经常用于没有先验经验的非线性可分问题。其中${\\sigma}$越小，每个数据点在高维空间越窄，形成的山峰越尖锐，对于相似度的要求更严格，越容易过拟合。\n",
    "\n",
    "  $$\\kappa(\\mathbf{x}_i, \\mathbf{x}_j) = \\exp\\left(-\\frac{\\|\\mathbf{x}_i - \\mathbf{x}_j\\|^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "- Polynomial Kernel 多项式核函数 :\n",
    "  \n",
    "  - It is a kernel function that maps the original space to a high-dimensional space, and is often used in the case of nonlinear separable problems with emprical experience. And the larger the ${d}$, the higher the dimension of the mapping, and the higher the risk of overfitting.\n",
    "\n",
    "  - 是将原空间映射到高维空间的核函数，经常用于有先验经验的非线性可分问题。其中${d}$越大，其映射的维度越高,过拟合的风险也越大。\n",
    "\n",
    "  $$\\kappa(\\mathbf{x}_i, \\mathbf{x}_j) = (\\alpha\\mathbf{x}_i^T \\mathbf{x}_j + c)^d$$\n",
    "\n",
    "- Sigmoid Kernel 双曲正切核函数 :\n",
    "\n",
    "  - It is a kernel function that maps the original space to a high-dimensional space, and is often used in the case of nonlinear separable problems with emprical experience. And the larger the ${\\alpha}$, the higher the dimension of the mapping.\n",
    "\n",
    "  - 是将原空间映射到高维空间的核函数，经常用于有先验经验的非线性可分问题。其中${\\alpha}$越大，其映射的维度越高。\n",
    "\n",
    "  $$\\kappa(\\mathbf{x}_i, \\mathbf{x}_j) = \\tanh(\\alpha\\mathbf{x}_i^T \\mathbf{x}_j + c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Most of the nonlinear separable problems can be well handled using the Gaussian kernel.\n",
    "\n",
    "大多数非线性分割使用高斯核都能很好处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.SMO(Sequential Minimal Optimization) Algorithm SMO算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By solving the above multi-dimensional vectors inner product calculating problem, the last problem is how to solve a set of optimal condition coefficients $a$ that satisfy the constraint conditions. \n",
    "\n",
    "通过核函数解决了高维向量内积计算问题后，就剩下如何求解出满足约束条件的一组最优条件系数$a$这最后一个问题了。\n",
    "\n",
    "Since the number of condition coefficients $a_i$ is the same as the number of vectors to be segmented, it is difficult to calculate the gradient lowest point directly, so the SMO algorithm is used, that is, the Sequential Minimal Optimization algorithm. \n",
    "\n",
    "由于条件系数$a_i$的个数与待分割向量一样多，难以直接计算梯度最低点，故而采用SMO算法，即序列最小优化（Sequential Minimal Optimization）算法。\n",
    "\n",
    "The idea is to select two condition coefficients to be optimized each time and fix the other coefficients, and then one coefficient can be eliminated through the constraint conditions, so that it becomes a quadratic function of one variable to find the optimal value problem. \n",
    "\n",
    "其思路是每次选择两个待优化条件系数而固定其他系数，由约束条件又可以化去一个系数，从而化成一个一元二次函数的求最值问题。\n",
    "\n",
    "For the optimal solution that has met the constraint conditions, when using the SMO algorithm for iterative optimization, the result will not change, so when all the condition coefficients have been traversed a few times through two optimizations, and each coefficient has no optimization space, the optimal result is obtained.\n",
    "\n",
    "对于已满足约束条件的最优解，再使用SMO算法做迭代优化时，是不会改变其结果的，所以当通过两两优化的方式把所有条件系数遍历几遍，各系数已不再有可优化空间后，即得到了最优结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 minimizing without constraints 无约束最小化 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, selecting $a_1$ and $a_2$ to be optimized, and ignoring other coefficients as the constant, and using the kernel function to replace the inner product of the vectors mapped to the high-dimensional space, the previous condition extreme value problem can be transformed into:\n",
    "\n",
    "首先选定$a_1$和$a_2$做优化，其他系数由于都先被视为常量，所以可以都先无视；并用核函数代替映射到高维空间的向量内积，则前述条件最值问题可化为：\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\min_{a} \\{\\quad & \\frac{1}{2} a_1^2 \\kappa(x_1, x_1) + \\frac{1}{2} a_2^2 \\kappa(x_2, x_2) + y_1 y_2 a_1 a_2 \\kappa(x_1, x_2) + y_1 a_1 \\sum_{i=3}^{m} y_i a_i \\kappa(x_i, x_1) + y_2 a_2\\sum_{i=3}^{m} y_i a_i \\kappa(x_i, x_2) - (a_1 + a_2)\\} \\\\\n",
    "\\text{s.t.} \\quad & a_1 y_1 + a_2 y_2 = \\sum_{i=3}^{m} a_i y_i = \\varsigma \\\\\n",
    "& 0 \\leq a_i \\leq C \\\\\n",
    "& i = 1,2,...,m\n",
    "\\end{align*}$$\n",
    "\n",
    "And $\\varsigma$ is a constant, $\\kappa(x_i, x_j)$ can be abbreviated as $\\kappa_{ij}$, so the above minimization objective function can be written as:\n",
    "\n",
    "其中$\\varsigma$为常数，$\\kappa(x_i, x_j)$可以简记为$\\kappa_{ij}$,因此上述要最小化目标函数可以记为：\n",
    "\n",
    "$$J = -\\frac{1}{2} a_1^2 k_{11} - \\frac{1}{2} a_2^2 k_{22} + y_1 y_2 a_1 a_2 k_{12} + y_1 a_1 \\sum_{i=3}^{m} y_i a_i k_{i1} + y_2 a_2 \\sum_{i=3}^{m} y_i a_i k_{i2} - (a_1 + a_2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $a_1y_1 + a_2y_2 = \\varsigma$, and $y_i$ can only take $±1$, it can be obtained that $a_1 = y_1(\\varsigma - a_2y_2)$, and substituting into the above equation to take the partial derivative of $a_2$:\n",
    "\n",
    "由于$a_1y_1+a_2y_2=\\varsigma$ ，且$y_i$只可能取$±1$，可得$a_1 = y_1(\\sigma-a_2y_2)$，代入上式对$a_2$求偏导为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial a_2} = (k_{11} + k_{22} - 2k_{12})a_2 + (y_2 k_{12} - y_1 k_{11})\\varsigma - y_2 \\sum_{i=3}^{m} y_i a_i k_{i1} + y_2 \\sum_{i=3}^{m} y_i a_i k_{i2} + y_1 y_2 - 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let above equation equal to 0, and we can get:\n",
    "\n",
    "令上式等于0，同时将待优化更新的$a_2$记为$a_2^{\\prime}$，区别于其他仍未更新的旧值$a_i$, 有：\n",
    "\n",
    "$$\n",
    "(k_{11} + k_{22} - 2k_{12})a'_2 = (y_2 k_{11} - y_2 k_{12}) \\mathcal{S} + y_2 \\sum_{i=3}^{m} y_i a_i k_{i1} - y_2 \\sum_{i=3}^{m} y_i a_i k_{i2} - y_1 y_2 + 1\n",
    "$$\n",
    "\n",
    "Since:\n",
    "由于：\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^m{y_i}{a_i}\\kappa_{ij} = a_1y_1\\kappa_{1j}+a_2y_2\\kappa_{2j}+\\sum_{i=3}^m{y_i}{a_i}\\kappa_{ij}\n",
    "$$\n",
    "\n",
    "So we can get:\n",
    "因此可得：\n",
    "\n",
    "$$\\begin{align*}\n",
    "(\\kappa_{11} \\kappa_{22} - 2\\kappa_{12})a'_2\n",
    "={(y_2 k_{11} - y_2 k_{12}) \\mathcal{S} + y_2 \\sum_{i=1}^{m} y_i a_i k_{i1} - a_1 y_1 k_{11} - a_2 y_2 k_{21}) - y_2 (\\sum_{i=1}^{m} y_i a_i k_{i2} - a_1 y_1 k_{12} - a_2 y_2 k_{22}) - y_1 y_2 + 1}\\\\\n",
    "={(y_2 k_{11} - y_2 k_{12}) \\mathcal{S} + y_2 \\sum_{i=1}^{m} y_i a_i k_{i1} - y_1 y_2 a_1 k_{11} - a_2 k_{21} - y_2 (\\sum_{i=1}^{m} y_i a_i k_{i2} + y_1 y_2 a_1 k_{12} + a_2 k_{22} - y_1 y_2 + 1}\n",
    "\\end{align*}$$\n",
    "\n",
    "To simplify the above equation, we can get:简化以上表达式，可得：\n",
    "\n",
    "$$E_j=\\sum_{i=1}^m y_i a_i \\kappa_{ij} - y_j$$\n",
    "\n",
    "即："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathbf{E} = \n",
    "\\begin{bmatrix}\n",
    "E_1 \\\\\n",
    "E_2 \\\\\n",
    "\\vdots \\\\\n",
    "E_m\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=1}^{m} y_i a_i k_{i1} - y_1 \\\\\n",
    "\\sum_{i=1}^{m} y_i a_i k_{i2} - y_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{i=1}^{m} y_i a_i k_{im} - y_m\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "k_{11} & k_{12} & \\dots & k_{1m} \\\\\n",
    "k_{21} & k_{22} & \\dots & k_{2m} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "k_{m1} & k_{m2} & \\dots & k_{mm}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "y_1 a_1 \\\\\n",
    "y_2 a_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_m a_m\n",
    "\\end{bmatrix} -\n",
    "\\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "\\vdots \\\\\n",
    "y_m\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so we can get:\n",
    "则有：\n",
    "\n",
    "$$y_2(E_1 - E_2) = y_2 \\sum_{i=1}^m y_i a_i \\kappa_{i1} - y_1 y_2 - y_2 \\sum_{i=1}^m y_i a_i \\kappa_{i2} + 1$$\n",
    "\n",
    "And we can get:\n",
    "代入前式可化简为：\n",
    "\n",
    "$$(k_{11} + k_{22} - 2k_{12})a'_2 = (y_2 k_{11} - y_2 k_{12})\\mathcal{S} - y_1 y_2 a_1 k_{11} - a_2 k_{21} + y_1 y_2 a_1 k_{12} + a_2 k_{22} + y_2 (E_1 - E_2)$$\n",
    "\n",
    "\n",
    "And we can get by sustituting $a_1 = y_1(\\varsigma - a_2y_2)$ into the above equation:\n",
    "\n",
    "再将$a_1 = y_1 (\\varsigma - a_2 y_2)$代入上式可得：\n",
    "\n",
    "$$\\begin{align*}\n",
    "(k_{11} + k_{22} - 2k_{12})a'_2 \n",
    "= y_2 k_{11}\\mathcal{S} - y_2 k_{12}\\mathcal{S} - y_2 (s - a_2 y_2) k_{11} - a_2 k_{21} + y_2 (s - a_2 y_2) k_{12} + a_2 k_{22} + y_2 (E_1 - E_2) \\\\\n",
    "= a_2 k_{11} - a_2 k_{21} - a_2 k_{12} + a_2 k_{22} + y_2 (E_1 - E_2) \\\\\n",
    "= (k_{11} + k_{22} - 2k_{12}) a_2 + y_2 (E_1 - E_2)\n",
    "\\end{align*}$$\n",
    "\n",
    "Therefore:因此：\n",
    "\n",
    "$$a'_2 = a_2 + \\frac{y_2 (E_1 - E_2)}{k_{11} + k_{22} - 2k_{12}}$$\n",
    "\n",
    "\n",
    "上式右侧$a_2$是未更新的旧值。此时得到的$a_2^\\prime$还并没有考虑约束条件$ 0≤a_i≤C$ ，因此还有待修剪。为方便区分，最终修剪后的各条件系数记为$\\hat a_i $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Clipping to satisfy the constraints 修建以满足约束"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结合约束条件$0≤a_i≤C$及$a_1 y_1 + a_2 y_2 = \\varsigma$ ，可确定$(\\hat a_1, \\hat a_2)$的取值范围落在如下范围内："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img-blog.csdnimg.cn/1a0f1ebf40d7489dad48bed092adb517.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAWFlfMDIwOQ==,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上可知，$\\hat a_2$的取值上下界范围为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{cases}\n",
    "\\text{if } (y_1 = y_2) & L = \\max\\{0, a_1 + a_2 - C\\} & H = \\min\\{a_1 + a_2, C\\} \\\\\n",
    "\\text{if } (y_1 \\neq y_2) & L = \\max\\{0, a_2 - a_1\\} & H = \\min\\{C, C + a_2 - a_1\\}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为上述$a_1$和$a_2$都是本轮更新前的原值，至此可得：\n",
    "\n",
    "$$\\begin{gather*}\n",
    "\\hat{a}_2 = \n",
    "\\begin{cases}\n",
    "H & \\text{if } \\hat{a}'_2 > H \\\\\n",
    "\\hat{a}'_2 & \\text{if } L \\leq \\hat{a}'_2 \\leq H \\\\\n",
    "L & \\text{if } \\hat{a}'_2 < L\n",
    "\\end{cases} \\\\\n",
    "\n",
    "\\hat{a}_1 = y_1 (S - \\hat{a}_2 y_2) = y_1 \\left[ (y_1 a_1 + y_2 a_2) - \\hat{a}_2 y_2 \\right] = a_1 + y_1 y_2 (a_2 - \\hat{a}_2)\n",
    "\\end{gather*}$$\n",
    "\n",
    "以此类推，将所有条件系数$a_i$遍历更新几遍后即得到最终优化后的一组条件系数$\\hat a_i$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由第五章可知，在拉格朗日函数中，对于不等式约束条件项，要么该约束条件表达式取0，要么该条件项前的条件系数取0（即所谓KKT条件），即若$\\hat a_s \\not= 0$， 则\n",
    "$$y_s(\\sum \\limits_{i=1}^m a_iy_i \\kappa_{is} + b) \\geq 1-\\xi_s $$\n",
    "也就是说该$x_s$向量要么是支持向量，即$\\xi_s=0$， 要么就是被容忍的异常向量，即$\\xi_s>0$。对于支持向量，有"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$b = y_s - \\sum_{i=1}^{m} a_i y_i K_{is} = -E_s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以我们只需找到支持向量就可求得决策面截距。同时上式也说明凡支持向量，其对应的偏移量$E$都是相等的，所以我们只需要找到所有条件系数不为零的特殊向量，再从这些特殊向量所对应的偏移量中找到出现次数最多的值，就是支持向量的偏移量，再取负数就得到了决策面的截距$\\hat b$， 最终得到的决策面方程为\n",
    "\n",
    "$$\\sum_{i=1}^{m} \\hat{a}_i y_i K(x_i, x) + \\hat{b} = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Application 应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 SVM Function code 函数代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib as matlib\n",
    "import numpy.random as random\n",
    "from collections import Counter\n",
    "\n",
    "# 定义高斯核函数\n",
    "def gaussKernel(X, Y, sigma=3):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.matrix\n",
    "        一个n行m列矩阵，代表m个n维向量。\n",
    "    Y : np.matrix\n",
    "        一个n行m列矩阵，代表另外m个n维向量。\n",
    "    sigma : float\n",
    "        调控参数。越小映射的空间维度约高。默认为3。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    K : np.matrix\n",
    "        一个m行1列矩阵，其中k_i = exp[-(||X_i-Y_i||^2)/(2*sigma^2)]。\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    X = np.matrix('1, 2, 3; 4, 5, 6').T \n",
    "    Y = np.matrix('1, 3, 5; 2, 4, 6').T\n",
    "    K = gaussKernel(X, Y, 3)\n",
    "    '''\n",
    "    K = np.exp(-(np.square(X - Y).T * matlib.ones((np.shape(X)[0], 1))) / (2*sigma**2))\n",
    "    return K\n",
    "\n",
    "# 基于SVM对一组做了二元标记的向量给出划分界限的决策平面\n",
    "def svnSimple(dataMatrix, labelVector, C, maxIter, kernel=None, sigma=3):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataMatrix : np.matrix\n",
    "        一个n行m列矩阵，代表m个n维向量，作为待分类向量。\n",
    "    labelVector : np.matrix\n",
    "        一个由+1和-1组成的1行m列矩矩阵，作为dataMatrix各列向量的标签。\n",
    "    C : float\n",
    "        惩罚系数。一个大于等于0的数值，越接近0则对异常向量的容忍度越高。\n",
    "    maxIter : int\n",
    "        最大遍历次数。对所有条件系数做一次迭代更新为一次遍历。\n",
    "    kernel : function\n",
    "        核函数。用于在低维空间计算映射到高维空间的向量内积。\n",
    "    sigma : float\n",
    "        核函数的调控参数。越小映射的空间维度约高。默认为3。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : np.matrix\n",
    "        一个1行m列矩阵，代表最优条件系数向量。\n",
    "    b : float\n",
    "        一个1行1列矩阵，代表决策平面的截距。\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    X = np.matrix('1, 1; 4, 3; 3, 3').T \n",
    "    Y = np.matrix('-1, 1, 1')\n",
    "    A, b = svnSimple(X, Y, 100, 10)\n",
    "    '''\n",
    "    n, m = np.shape(dataMatrix)         # 初始化待分向量维度和向量个数\n",
    "    A = matlib.zeros((1, m))            # 初始化条件系数向量\n",
    "    \n",
    "    # 初始化待分向量间内积矩阵\n",
    "    if callable(kernel):                # 如果给定了核函数则使用核函数做高维内积\n",
    "        K = matlib.zeros((m, m))        # 初始化高维内积矩阵\n",
    "        for i in range(m):\n",
    "            K[:,i] = kernel(dataMatrix[:,i] * matlib.ones((1,m)), dataMatrix, sigma)\n",
    "    else:\n",
    "        K = dataMatrix.T * dataMatrix\n",
    "\n",
    "    # 由SMO算法迭代出所有最优条件系数\n",
    "    iterNum = 0                         # 初始化迭代次数\n",
    "    effTraversalNum = 0                 # 初始化有效遍历次数\n",
    "    while (iterNum < maxIter):\n",
    "        alphaPairsChanged = 0           # 初始化条件系数更新次数\n",
    "        for i in range(m):\n",
    "            # 计算各待分向量到决策面的偏移值（不含b影响）向量\n",
    "            E = (K*np.multiply(A, labelVector).T ).T- labelVector\n",
    "            # 从其他条件系数中再随机选出一个与当前条件系数作为一对待优化系数\n",
    "            j = i\n",
    "            while (j == i):\n",
    "                j = int(random.uniform(0, m))\n",
    "            # 计算当前待优化的第二个条件系数a2的待修剪值\n",
    "            a2 = A[0,j] + labelVector[0,j]*(E[0,i]-E[0,j])/(K[i,i]+K[j,j]-2*K[i,j])\n",
    "            # 计算a2的上下界\n",
    "            if labelVector[0,i] == labelVector[0,j]:\n",
    "                l = max(0, A[0,i]+A[0,j]-C)\n",
    "                h = min(C, A[0,i]+A[0,j])\n",
    "            else:\n",
    "                l = max(0, A[0,j]-A[0,i])\n",
    "                h = min(C, C+A[0,j]-A[0,i])\n",
    "            # 修剪条件系数a2\n",
    "            if a2 > h:\n",
    "                a2 = h\n",
    "            elif a2 < l:\n",
    "                a2 = l\n",
    "            # 当a2更新变化太小时便无需更新\n",
    "            if (abs(A[0,j] - a2) < 0.00001): continue\n",
    "            # 计算条件系数a1\n",
    "            a1 = A[0,i] + labelVector[0,i]*labelVector[0,j]*(A[0,j]-a2)\n",
    "            # 更新条件系数向量\n",
    "            A[0,i] = a1\n",
    "            A[0,j] = a2\n",
    "            # 统计本此遍历中条件系数更新次数\n",
    "            alphaPairsChanged += 1\n",
    "        # 统计有效遍历次数\n",
    "        if alphaPairsChanged != 0: effTraversalNum += 1\n",
    "        # 遍历迭代次数加1\n",
    "        iterNum += 1\n",
    "    print(\"共完成有效遍历%d次。\" %effTraversalNum)\n",
    "    \n",
    "    # 通过支持向量计算决策平面截距b\n",
    "    spVecIndex = []                            # 初始化特殊向量序号集合\n",
    "    for k in range(m):\n",
    "        if abs(A[0,k]) > 0.01:                 # 条件系数显著大于零对应的是特殊向量\n",
    "            spVecIndex.append(k)\n",
    "    spE = E[:, spVecIndex].tolist()[0]         # 获取所有特殊向量偏移值\n",
    "    roundSpE = []                      \n",
    "    for n in spE: roundSpE.append(round(n, 4)) # 精确到小数点后4位\n",
    "    # 对各偏移量按出现次数由大到小排序\n",
    "    listRoundSpE = list(Counter(roundSpE).items())\n",
    "    listRoundSpE.sort(key=lambda x:x[1],reverse=True)\n",
    "    b = -listRoundSpE[0][0]                    # 出现次数最多的E对应支持向量\n",
    "    return A, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Plotting code 绘图代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib as matlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 绘制待分二维向量散点图及其分界线（如有）\n",
    "def showDataSet(dataMatrix, labelVector, A=None, b=None, kernel=None, sigma=None):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataMatrix : np.matrix\n",
    "        一个2行m列矩阵，代表m个2维向量，作为待分类向量。默认第一行是横坐标，第二行是纵坐标。\n",
    "    labelVector : np.matrix\n",
    "        一个由+1和-1组成的1行m列矩矩阵，作为dataMatrix各列向量的标签。\n",
    "    A : np.matrix\n",
    "        一个1行m列矩阵，代表各待分向量对应的条件系数，非零系数对应支持向量。默认为空，\n",
    "        即不画分界线。\n",
    "    b : float\n",
    "        一个1行1列矩阵，代表决策平面的截距。默认为空，即不绘制分界线。\n",
    "    kernel : function\n",
    "        核函数。用于在低维空间计算映射到高维空间的向量内积。为空则默认为原向量空间内积。\n",
    "    sigma : float\n",
    "        核函数的调控参数。越小映射的空间维度约高。\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    null\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    只能绘制二维向量散点图。\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    X = np.matrix('1, 1; 4, 3; 3, 3').T \n",
    "    Y = np.matrix('-1, 1, 1')\n",
    "    A = np.matrix('0.25, 0, 0.25')\n",
    "    b = np.matrix('-2')\n",
    "    showDataSet(X, Y, A, b)\n",
    "    '''\n",
    "    n, m = np.shape(dataMatrix)               # 初始化待分向量维度和向量个数\n",
    "    if not n == 2:                            # 校验向量维度只能是2维\n",
    "            raise Exception(\"only 2-dimension vectors can be darwn\")\n",
    "    plusColumNum = []                         # 正向量列号集合\n",
    "    minusColumNum = []                        # 负向量列号集合\n",
    "    for i in range(m):\n",
    "        if labelVector[0,i] > 0:              # 如果标签为正\n",
    "            plusColumNum.append(i)            # 将列号归入正向量列集合\n",
    "        else:\n",
    "            minusColumNum.append(i)           # 否则将列号归入负向量列集合\n",
    "    plusData = dataMatrix[:, plusColumNum]    # 由正向量列号获取组成正向量矩阵\n",
    "    minusData = dataMatrix[:, minusColumNum]  # 由负向量列号获取组成负向量矩阵\n",
    "    plt.figure()\n",
    "    plt.axis('scaled')                        # 横纵坐标尺度一致（即使在窗口缩放时）\n",
    "    x_min = min(min(dataMatrix[0].tolist()[0])-1, 0)  # X轴下限\n",
    "    x_max = max(max(dataMatrix[0].tolist()[0])+1, 0)  # X轴上限\n",
    "    y_min = min(min(dataMatrix[1].tolist()[0])-1, 0)  # Y轴下限\n",
    "    y_max = max(max(dataMatrix[1].tolist()[0])+1, 0)  # Y轴上限\n",
    "    plt.xlim([x_min, x_max])                  # 设置x轴坐标系范围\n",
    "    plt.ylim([y_min, y_max])                  # 设置y轴坐标系范围\n",
    "    plt.scatter(plusData[0].tolist()[0], plusData[1].tolist()[0])   #正向量散点图\n",
    "    plt.scatter(minusData[0].tolist()[0], minusData[1].tolist()[0]) #负向量散点图\n",
    "    # 移动坐标系\n",
    "    ax = plt.gca()                               # 获取当前坐标系\n",
    "    ax.spines['right'].set_color('none')         # 右边框设置成无颜色\n",
    "    ax.spines['top'].set_color('none')           # 上边框设置成无颜色 \n",
    "    ax.spines['bottom'].set_position(('data',0)) # x轴在y轴，０的位置\n",
    "    ax.spines['left'].set_position(('data',0))   # y轴在x轴，０的位置\n",
    "    # 绘制分界线\n",
    "    if b is not None:                            # 如果条件系数向量非空\n",
    "        x = np.linspace(x_min, x_max, 100)\n",
    "        y = np.linspace(y_min, y_max, 100)\n",
    "        meshX,meshY = np.meshgrid(x,y)           # 对向量显示区域网格化\n",
    "        vecX = matlib.zeros((2, 1))              # 初始化决策面方程自变量向量\n",
    "        Z = matlib.zeros((100, 100))             # 初始化曲面高度坐标\n",
    "        for i, item_x in enumerate(x):\n",
    "            for j, item_y in enumerate(y):\n",
    "                vecX[0] = item_x\n",
    "                vecX[1] = item_y\n",
    "                matX = vecX * matlib.ones((1, m))\n",
    "                if callable(kernel):             # 如果给定了可调用的核函数\n",
    "                    # 使用核函数计算每个网格点的坐标与所有乘过条件系数和标签值后的待\n",
    "                    # 分向量的高维内积，再加截距即得该网格点上的曲面高度\n",
    "                    Z[j,i] = kernel(dataMatrix, matX, sigma).T * np.multiply(labelVector, A).T +b\n",
    "                else:                            # 否则直接在当前二维空间做内积\n",
    "                    # 此处务必注意i是行号，其实是y坐标；j是列号，其实是x坐标\n",
    "                    Z[j,i] = (dataMatrix.T * vecX).T * np.multiply(labelVector, A).T +b\n",
    "        plt.contour(meshX, meshY, np.array(Z), [0], colors='r')\n",
    "    # 标注支持向量和异常向量\n",
    "    if A is not None and np.shape(A)[1] == m:    # 如果条件系数向量非空\n",
    "        svOder = []                              # 特殊向量序号集合\n",
    "        for i in range(m):\n",
    "            if abs(A[0,i]) > 0.01:               # 条件系数显著大于0的才是特殊向量\n",
    "                svOder.append(i)\n",
    "        svSet = dataMatrix[:, svOder]            # 特殊向量集合\n",
    "        plt.scatter(svSet[0].tolist()[0], svSet[1].tolist()[0], s=150, \n",
    "                    c='none', alpha=0.7, linewidth=1.5, edgecolor='red')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Linearly Separable effect 线性分隔效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m Y1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mc_[plusY, minusY]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 使用SVM模型给出决策平面的法向量和截距，以及条件系数向量\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m A, b \u001b[38;5;241m=\u001b[39m \u001b[43msvn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 绘制待分向量散点图和分界线\u001b[39;00m\n\u001b[0;32m     17\u001b[0m showDataSet(X1, Y1, A, b)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib as matlib\n",
    "import numpy.random as random\n",
    "\n",
    "# 设定待分类点的坐标及其标签\n",
    "plusX = np.matrix(random.standard_normal((20, 2))).T + np.matrix('1; 1')\n",
    "plusY = matlib.ones((1,np.shape(plusX)[1]))\n",
    "minusX = np.matrix(random.standard_normal((25, 2))).T + np.matrix('5; 5')\n",
    "minusY = matlib.ones((1,np.shape(minusX)[1]))*-1\n",
    "X1 = np.c_[plusX, minusX]\n",
    "Y1 = np.c_[plusY, minusY]\n",
    "\n",
    "# 使用SVM模型给出决策平面的法向量和截距，以及条件系数向量\n",
    "A, b = svnSimple(X1, Y1, 50, 50)\n",
    "# 绘制待分向量散点图和分界线\n",
    "showDataSet(X1, Y1, A, b)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
